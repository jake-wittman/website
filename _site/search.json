[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jake Wittman",
    "section": "",
    "text": "My research attempts to answer questions about the basic biology in invasive forest insect pests to help us better manage these pests. I do this through a combination of field and laboratory experiments. Specifically, my research for my PhD has focused on emerald ash borer, a small invasive beetle in North America that is killing our ash trees. I enjoy translating the results of my research into actionable information for land managers and other stakeholders that deal with invasive insects.\nEarly in grad school, I discovered I loved statistics and programming. Having access to the tools used to identify patterns in the noisy data we often work with in ecology was so enlightening! I sought out opportunities to learn more about statistics and ended up with a graduate minor in biostatistics (basically, almost all the coursework required for a MSc in biostatistics and no thesis component). I work at the intersection of ecology, entomology, and management. The data we collect is often messy and requires significant cleaning and thoughtful analysis, where my biostatistics background has come through in spades. I’ve applied my statistical abilities in my PhD to address some outstanding questions related to the management of emerald ash borer (see my Projects page for more).\nI’m also very passionate about education. Before starting graduate school, I taught 7 - 12 grade science for two years. I’ve continued to seek out formal and informal education opportunities, ranging from working as a teaching assistant in undergraduate and graduate courses while in graduate school, volunteering at workshops, providing R stats advice to colleagues, and a little bit of private tutoring.\nIn my free time, I enjoy running, playing my trombone, video games, reading, hiking/camping, and working on small coding projects."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Working through Danielle Navarro’s workshop on generative art\n\n\n\n\n\n\nAug 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn a bit more Python and make my scraping more reliable, I rewrote my scraping and database building scripts in Python.\n\n\n\n\n\n\nJan 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI fed a bunch of caterpillars different types of food to see how it would affect their movement\n\n\n\n\n\n\nJan 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr how I determined what minimum temperature would kill a parasitoid wasp we use to control the invasive beetle emerald ash borer\n\n\n\nJake Wittman\n\n\nJan 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I leveraged my programming knowledge to avoid paying real money for a video game\n\n\n\n\n\n\nDec 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJust here to kick the tires\n\n\n\nJake Wittman\n\n\nNov 13, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-11-13_hello-world/index.html",
    "href": "posts/2021-11-13_hello-world/index.html",
    "title": "Hello world!",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(magrittr)\n\nWarning: package 'magrittr' was built under R version 4.0.5\n\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(avg_mpg = mean(mpg))\n\n# A tibble: 3 x 2\n    cyl avg_mpg\n  <dbl>   <dbl>\n1     4    26.7\n2     6    19.7\n3     8    15.1\n\n\n\nggplot(mtcars, aes(x = hp, y = mpg, colour = cyl)) +\n  geom_point() +\n  scale_color_viridis_c()"
  },
  {
    "objectID": "posts/2021-12-03_time-is-money-friend/index.html",
    "href": "posts/2021-12-03_time-is-money-friend/index.html",
    "title": "Downloading data from the World of Warcraft Auction House",
    "section": "",
    "text": "Upon my return, I found the game had changed quite a bit! One piece that was new to me, and that I found instantly intriguing, was that you could now buy game time using in game gold! For the uninitiated, World of Warcraft is a subscription based game - you have to pay $15/month to play (in the USA anyway, prices are different in different locations). Being able to buy game time with in-game currency was a game changer for me. One of the reasons I quit playing WoW 10 years ago was because I didn’t feel like I could justify paying 15 dollars a month when I was a poor undergrad. Now that I’m an only-very-slightly-less poor grad student, 15 dollars a month is still a fair bit of change but if I could pay for it with gold I earned while playing a game I love, that’s great! As the goblin characters in WoW will tell you: “Time is money, friend!”.\nI immediately got to work learning about how others were successful in translating their play time into game time. A lot of players will make their in-game gold playing the Auction House. In WoW, the Auction House is a place where players can (surprise!) auction off in-game items in exchange for currency. There are a variety of professions that players can choose from to gather or produce goods that are useful for other players. Blizzard (the game developer) also happens to have an API that allows you to access the data for all the auctions on a realm’s Auction House, and this API is updated every hour. Services exist that tap into this API to help understand the price trends of items for the economically-minded player, but I wanted to level-up my programming skills a bit and build myself a tool that did exactly what I needed. So, I set about learning how to access this API, built a database of auctions, and set up some email alerts for items I was watching so I could buy low and sell high! Since I built this program, I’ve earned about 8 million in-game gold, which translates to roughly $600 worth of ‘Blizzard Balance’ (roughly because the gold cost of a 15 dollar token fluctuates over time). That’s more than enough money for me to pay for my game time, plus more for me to buy time for friends or spend it on other Blizzard games.\nHere I’ll walk through the code I used to do this. I wrote the script in an R markdown file so I could weave together R code chunks with Python code chunks. I used both languages in part because I want to learn more Python, so I thought why not, but also because I was having trouble accessing the API using R. Setting it up in Python worked almost flawlessly. I’m a total novice when it comes to scraping APIs, so I’m sure there was something I was doing wrong in R. You’ll also see that I use renv to help manage both the R and Python environments - something else I’m learning more about! I’ve already run into dependency hell with a few R projects, so I’m very thankful for the renv package.\n\nSys.time()\n\n# R imports\n.libPaths(\"path/to/your/project/renv\")\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(googledrive)\nlibrary(reticulate)\nlibrary(lubridate)\nlibrary(emayili)\nuse_condaenv(\"path/to/your/conda/env\")\n\n`%!in%` <- Negate(`%in%`)\n\n# I keep my API keys in this file (make sure you add this file to a .gitignore if you're using git!)\nsource('data/keys.R')\n\nHere I set up the Python functions I’ll need to get to the API. If you want to do this, you’ll need to make sure you have your region specified.\n\n# Set up python functions\nimport requests\nfrom datetime import datetime\nimport pandas as pd\n\n# Create a new access token\ndef create_access_token(client_id, client_secret, region = \"us\"):\n   data = { 'grant_type': 'client_credentials' }\n   response = requests.post('https://%s.battle.net/oauth/token' % region,\n                            data = data,\n                            auth = (client_id, client_secret))\n   return response.json()\n\n# Get Malf and connected realms data\ndef get_malfurion(search):\n   search = search\n   response = requests.get(search)\n   return response.json()[\"auctions\"]\n \nresponse = create_access_token(r['client_id'], r['client_secret'])\ntoken = response['access_token']\n\nNow, I specify a few more parameters including my realm ID. I’m on a connected realm for Malfurion (+ a bunch of others I never remember). Each of our realms returns it’s own ID, but for some reason this id (1175) was the only one that actually returned data. So if you’re on a connected realm, you might have to experiment using other realm’s ids.\n\n# Set up search query with Malfurion realm ID\nid <- 1175\ntoken <- py$token\nsearch <- glue(\"https://us.api.blizzard.com/data/wow/connected-realm/{id}/auctions?namespace=dynamic-us&locale=en_US&access_token={token}\")\n\nNow we can actually access the data! I add a few features to the data to capture when I pulled the data. I also write the file to csv. I wrote the file to csv because, even though I should be able to access a Python data frame in R through the reticulate package, I was having trouble getting it to work. My work around was to write the file, reload it so R had access to it, write the R data frame to a SQL database, then delete the csv. You can also see that I do a little bit of cleaning with the data. Depending on the type of item posted, it either has a unit_price or a buyout, but I want just a single column for cost. I then convert the cost from how it is listed in units of bronze to units of gold (100 bronze in a silver, 100 silver in a gold).\n\nauction_data = get_malfurion(r['search'])\nauction_data = pd.DataFrame(auction_data)\n# Expand the item column\nauction_data = auction_data.rename(columns={\"id\": \"auction_id\",})\nauction_data = pd.concat([auction_data.drop(['item'], axis=1), auction_data['item'].apply(pd.Series)], axis=1)\n\n# Drop 'bonus_list' and 'modifiers' \n#   These are subgroups of an equipable item with the bonus stats (intellect agility, strength, etc)\nauction_data['collection_year'] = datetime.now().strftime('%Y')\nauction_data['collection_month'] = datetime.now().strftime('%m')\nauction_data['collection_day'] = datetime.now().strftime('%d')\nauction_data['collection_hour'] = datetime.now().strftime('%H')\nfilename = datetime.now().strftime('F:/Documents/WoWAH/data/Malfurion_NA-%Y-%m-%d-%H-%M.csv')\ntablename = datetime.now().strftime('Malfurion_NA-%Y-%m-%d-%H-%M')\nauction_data.to_csv(filename, index = False)\n\n\n# Read the data into R to clean it a bit more (since I'm less good at cleaning)\n# in python\nauction_df <- read_csv(py$filename)\n\n# Unit prices are for stackable items \nauction_df <- auction_df %>% \n   mutate(unit_price = replace_na(unit_price, 0),\n          buyout = replace_na(buyout, 0),\n          cost = unit_price + buyout,\n          cost_g = cost / 10000)\n\nwrite_csv(auction_df, py$filename)\n\n# Add to database\nlibrary(RSQLite)\nlibrary(dbplyr)\n\ncon <- dbConnect(RSQLite::SQLite(), \"data/WoWAH_database.sqlite\")\ndbWriteTable(con, py$tablename, auction_df)\nfile.remove(py$filename) # delete the csv file once it is in the database.\n\nThe next step was to query the API for information about each item on the auction house. The auction house data doesn’t contain the human-readable name for the item, it just has an item ID number. So I have to use a seperate query to get the item information. To do this efficiently and prevent constant excessive querying of the API, I built a new data base table to hold this information and then build on this table if I ever find an item in my most recent auction house scan that isn’t in the item info database.\nTo do this, I first start by finding all the item ids that aren’t already in the data base. Then, I split this list of item ids into chunks with a max of 50 item ids per chunk. I found this number through some trial and error querying the API - any queries requesting information on more than 50 items at a time only returned a max of 50 items. Then I construct as many queries as there are chunks. The first time I ran this, it took a while because I was building the item information database from scratch. But it’s pretty uncommon for previously unseen items to show up in the auction house, so this doesn’t actually run that often anymore.\n\n# Check for any items in the newly scraped AH data that isn't in\n# the current database\nitem_db <- tbl(con, \"item_name\")\nauction_df <- tbl(con, py$tablename)\nnot_in_db_df <- anti_join(auction_df, item_db, by = \"id\") %>% \n   collect()\nid <- 1175\ntoken <- py$token\n# Format item info to request from API\nitem_ids <- unique(not_in_db_df$id)\n\n# Helper function to chunk id vector\ncreateChunks <- function(x, elements.per.chunk){\n   # plain R version\n   split(x, rep(seq_along(x), each = elements.per.chunk)[seq_along(x)])\n\n}\n\nitem_chunks <- createChunks(item_ids, 50)\nsearch_chunks <- map(item_chunks, function(.x) {\n   id_char <- paste(.x, collapse = \"||\")\n   glue(\"https://us.api.blizzard.com/data/wow/search/item?namespace=static-us&locale=en_US&orderby=id&&_pageSize=1000&id={id_char}&_&access_token={token}\")\n})\n# Need this to be a Python list so I remove names (a named list becomes a python dict)\nsearch_chunks <- unname(search_chunks)\n\nNow that I have the item-query-chunks, I use some more Python code to actually do the query.\n\nimport pandas\nitem_names = []\n\nfor search_url in r['search_chunks']:\n   response = requests.get(search_url).json()\n   response_df = pandas.io.json.json_normalize(response, record_path = ['results'])\n   item_names.append(response_df)\n\n\nitem_df = pd.concat(item_names, ignore_index = True, axis = 0, sort = True)\n\nAnd now I clean up the item table a bit and then write it/append it to the database.\n\n# Get Item classes index an\nnew_item_df <- py$item_df %>% \n   select(data.id, data.is_equippable, data.is_stackable, data.level, data.max_count,\n          data.media.id,  data.name.en_US,\n          data.purchase_price, data.required_level, data.sell_price) %>% \n   rename_all(~stringr::str_replace(., \"^data.\", \"\")) %>% \n   rename_at(.vars = vars(ends_with(\".en_US\")),\n             ~ sub('[.]en_US$', '', .))\n\ndbAppendTable(con, name = \"item_name\", value = new_item_df)\n\nFor a while I was running this on a computer with limited storage, so I set it up to delete any data older than 4 weeks. I’ve since turned off this code chunk, as I moved the script over to a computer with much more storage so I’m not really concerned about the amount of data I’m keeping.\n\n# I don't need to keep all the data, let's keep a month's worth\ntable_list <- data.frame(table_list = dbListTables(con))\ntable_list <- filter(table_list, table_list %!in% c(\"item_name\", \"item_db\"))\ntable_list$date_times <-\n   str_extract(table_list$table_list,\n               \"[0-9]{4}-[0-9]{2}-[0-9]{2}-[0-9]{2}-[0-9]{2}\")\ntable_list$date_times <- ymd_hm(table_list$date_times)\n# Just in case the tables aren't in order for date time\ntable_list$index <- 1:nrow(table_list)\ntable_list <- arrange(table_list, desc(date_times))\nkeep_tables <- slice(table_list, 1:672)\nremove_tables <- table_list[table_list$index %!in% keep_tables$index, ]\nremove_tables <- remove_tables$table_list\n\nmap(remove_tables, ~dbRemoveTable(con, .x))\ndbDisconnect(con)\n\nLastly, I set up some email alerts! Here I just have an example for two items I was watching: elethium ore and widowbloom. I had to be careful with this, as I initially set the alerts to trigger at prices that occurred to frequently and Google flagged my account for inappropriate use! There’s probably some other service I could use and not worry about it, but I had the threshold set too high anyway and was getting more emails than I could act on.\n\nwatched_items <- c('Elethium Ore', 'Widowbloom')\nwatched_item_df <- auction_df %>% \n   left_join(item_db, by = 'id', copy = TRUE) %>% \n   filter(name %in% watched_items) %>% \n   select(name, cost_g, quantity) %>% \n   group_by(name, cost_g) %>% \n   summarise(quantity = sum(quantity, na.rm = TRUE)) %>% \n   collect()\n\nemail_items <- watched_item_df %>% \n   group_by(name) %>% \n   summarise(min_cost = min(cost_g)) %>%\n   left_join(., watched_item_df, by = c('name', 'min_cost' = 'cost_g')) %>% \n   mutate(buy = case_when(name == 'Elethium Ore' & min_cost < 60 ~ 1,\n                          name == 'Widowbloom' & min_cost < 16 ~ 1,\n                          TRUE ~ 0)) %>% \n   filter(buy == 1)\nitems_to_buy <- paste0(email_items$name, \": \", email_items$quantity, collapse = \", \")\nnames(items_to_buy) <- 'items_to_buy'\n# Set up text to send the giver their assigned gift recipient\nif (nrow(email_items) > 0) {\nbody <- \"Dear Jake,\n\n<p>Consider purchasing the following items:\n<br> {items_to_buy}\n\n\"\n\n# Set up email\nemail <- envelope()\nemail <- email %>% \n   from(\"your.account.for.sending.alerts@email.com\") %>% \n   to('your.account.for.receiving@emailcom')\n\nemail <- email %>% \n   subject(paste0('WoW AH ', Sys.time()))\n\nemail <- email %>% \n   html(glue_data(items_to_buy, body))\n\n\nsmtp <- server(host = \"smtp.gmail.com\",\n               port = 587,\n               username = \"your.account.for.sending.alerts@email.com\",\n               password = \"your.password.for.email.authentication\")\nsmtp(email, verbose = TRUE)\n}\n\nAnd that’s it! Hopefully there’s enough information here for someone else to get a similar system up and running. Next steps I’d like to do are to build a Shiny dashboard for visualizing some of this data. I’m also interested in learning more about time series analysis, so I’m planning to use this data in some self-teaching exercises. Reach out to me on twitter if you have questions!"
  },
  {
    "objectID": "posts/2022-01-11_how-cold-is-too-cold/index.html",
    "href": "posts/2022-01-11_how-cold-is-too-cold/index.html",
    "title": "How cold is too cold for a wasp?",
    "section": "",
    "text": "Tl;dr: A wasp we use to control populations of an invasive beetle in North America should be able to survive most winters in the northern range of locations it would be released. This is good, because it’s potential ability to survive in the north is part of the reason it was selected as a biological control agent.\nIf you live in the eastern half of the United States and are a fan of trees, you’ve probably heard about emerald ash borer. This little jewel beetle is destroying ash trees here in the United States. It arrived in Michigan in the 1990s and was first detected in the early 2000s. It was most likely introduced on some solid wood packing material brought over from it’s native range in eastern Asia. The beetle is an insidious killer, causing virtually 100% ash mortality in areas where it has spread to in North America. The beetle spends the majority of it’s life feeding as a larva underneath the bark of ash trees, which makes it very difficult to detect early on. It is often not detected for several years, at which time ash trees will start to shows signs of infestation. At this point though, the beetle populations will be so large and spread out eradication is virtually impossible.\n\n\n\nEmerald ash borer is an invasive beetle destroying ash trees in North America. Image from Pennsylvania Department of Conservation and Natural Resources\n\n\nOne tool we have to combat the spread of this insect is a classical biological control program. Biological control (often abbreviated just as biocontrol) programs make use of, you may have guessed, biology to tackle pest issues. In this case, we’re talking about using predators or parasites from the native range of emerald ash borer. You may recall learning about biocontrol in your high school biology class. Examples from the past where BIG mistakes were made in the judicious movement of predators to novel ecosystems are often used to highlight the danger of moving organisms around without considering how they’ll interact with the organisms that live in the target destination. Cane toads released in Australia in the 1930s to deal with a cane beetle problem have become the pests, rather than dealing with the pests they were supposed to suppress. These toads are not specific predators; they eat many plantas and insects beyond just the cane beetle. We’ve learned a lot of lessons from these failed biocontrol attempts. In many places, including the US, any potential biocontrol agent has to go through extensive testing to show that it will not have off-target effects. In other words, they have to show that any native organisms related to the target pest will not be attacked by the biological control agent.\nIn the USA, four wasps native to eastern Asia that attack emerald ash borer have been tested and approved for release as part of the biocontrol program. These wasps are parasitoids, which are similar to parasites in that they live on a host organism, but parasitoids will always ultimately kill their host. These wasps are very small and do not sting humans. They cannot eradicate emerald ash borer but they (hopefully) will cause enough mortality to slow the growth of the beetle’s populations, which in turn will slow its spread and how quickly trees in invaded areas die. This gives management agencies more time to react, as trees killed by this beetle need to be removed or important trees need to be identified and treated with insecticide injections.\n\n\n\nSpathius galinae, one of four parasitoid wasps used in the biological control program for emerald ash borer in North America. Image from Jian Duan (USDA-ARS Beneficial Insects Introduction Research Unit) - https://www.flickr.com/photos/usdagov/14053044929.\n\n\nOne of these wasps, Spathius galinae, was the focus of one of my dissertation chapters. This tiny wasp was the last of the four biological control agents to be approved for release in the US. It’s currently being released in locations north of 40° latitude as a replacement for Spathius galinae, one of the other parasitoids. Spathius galinae populations were not performing well north of 40°, likely because the colder winters caused a mismatch in the development rate of this insect and emerald ash borer, so that it was not active when it would be most effective. Spathius agrili is from a colder region of eastern Asia in Russia, so should be better suited to the more northern climates. Some work has been done to see how well this insect will survive winters in the northeastern United States, but none had been done in the upper Midwest where winters can get extremely cold. I set out to answer that question.\n\n\n\nRelease locations for Spathius galinae and Spathius agrili in the United States and Canada. Underlying map from Duan et al. 2018 Progress and Challenges of Protecting North American Ash Trees from the Emerald Ash Borer Using Biological Control\n\n\nMany insects native to areas where it gets cold in the winter have developed behavioral or physiological mechanisms to handle the cold, however they all have their limits. We wanted to know what temperatures would kill Spathius galinae so we could determine if there was anywhere in North America where the cold would limit its effectiveness. We started with some labortory experiments to get some baselines in a nice controlled environment.\n\n\n\nThe silicon chiller bath we used to control how quickly the insects were chilled.\n\n\nSpathius galinae spend the winter in an immature form, known as a pre-pupae. We had some of these sent to us from a research lab where these insects are reared and we put them in the silicon chiller bath (pictured above). This bath allows us to cool the insects down to temperatures as low as -40 °F (although in practice, it has difficulty getting below -25 °F) at a controlled rate so we can observe them. The wires coming out of those test-tubes are thermocouples, which we use to measure the actual temperature of each insect. When we are cooling these insects, we choose a range of temperatures to cool them down to and we look for ‘exotherms’. When the water freezes, it releases a little bit of heat which can be picked up by these thermocouples. This release of heat is called an exotherm. The exotherm is important to look for as it determines the insects ‘supercooling point’.\n\n\n\nA discolored Spathius galinae prepupae. This one was killed b y the chilling process and became discolored. Normally, they’re a nice creamy-white.\n\n\n\n\n\nWhen water freezes, it releases a bit of heat which can be picked up by our thermocouples. The insects in this image are shown freezing around -26 °C (-15 °F).\n\n\nI mentioned before that insects have evolved mechanisms for dealing with the cold and supercooling is one of those. Many insects produce anti-freeze compounds that prevent the water in their cells from freezing. Freezing water inside of cells is bad bad bad - it can rupture the cell walls and kill them. By supercooling their body liquids, some insects are able to survive sub-zero temperatures. (Many insects can supercool, but supercooling doesn’t always save them. They can be killed via other pathways even if they don’t freeze.) Insects that supercool and do not die before they freeze are termed ‘chill tolerant’. Other insects are freeze tolerant; they are able to survive freezing. Measuring the supercooling point (when the insect freezes) and then transferring them to an environment where we can see if they’ll resume development and living is a way to determine if an insect is freeze tolerant or not. By cooling them to a range of lower temperatures, we can also get a baseline for how many insects we would expect to survive given minimum temperatures in the winter.\nWe measured both the distribution of supercooling points, which gives us an idea of what proportion of the population freezes at progressively lower temperatures, and then we looked at how many of the insects were able to complete development and emerge as adults after being chilled to this low temperature. (Aside: the insects were chilled to the designated low temperature, then immediately placed in a 0 °C (32 °F) container until we could move them to incubators. This is called a lower lethal temperature experiment; we’re trying to find the lower temperature such that a brief exposure kills them. This is in contrast to a lower lethal time temperature, where an insect is held at a designated low temperature for various periods of time to see how longer exposures affect mortality. Both are measures of an insects cold tolerance, but we focused on lower lethal temperature for this part of the study.) We fit a logistic regression to the lower lethal temperature data as a model of what proportion of insects we would expect to become adults given exposure to a particular low temperature.\n\n\n\n\n\nDistribution of supercooling points observed in larvae of Spathius galinae (n = 73) cooled in a silicon cooling bath.\n\n\n\n\n\n\n\n\n\nProbability that larvae of Spathius galinae (n = 427) would eclose after being cooled in the laboratory to a sub-zero temperature. Points are the Abbott corrected proportion of S. galinae that eclosed (1 = 100% eclosion, 0 = 0% eclosion) at a given temperature, while error bars are ± two standard errors. The solid line are predicted response rates from the models fit to these data. The grey band represents the 95% confidence interval for the fitted line.\n\n\n\n\nSweet! We have a nice cumulative freezing curve - we can see that about 50% of the population is expected to freeze at about -25 °C (-13 °F) (remember, freezing doesn’t necessarily equal death. We’re looking to see if any of these insects that froze will continue to develop). The resulting data are kind of messy (why such a low proportion emerging at around -6 °C? Why so many surviving at -30 °C?), but we see a clear trend that as temperatures drop, more and more of the insects are dying as they don’t finish development. What isn’t clear from this graph though is that a few of the insects that did freeze did survive - we’ll come back to that later.\nFor the second part of this study, we went out to the field to see how the insects would fare in the real world. We took some ash log sections which contained emerald ash borer under the bark that had been parasitized by Spathius galinae out to three locations in Minnesota: the University of Minnesota campus in Saint Paul, a research station in the west central part of the state near Morris, MN and further south of Saint Paul near Waseca, MN. We placed half our logs at each location under the snow to provide them with a more stable and insulated environment and tied the other half to the trunk of trees above the snowline. We left them out over the winter of 2019-2020 to be exposed to the elements! We were going to come back in May 2020 to grab them, but due to the brand new covid-19 pandemic and upcoming University travel restrictions, we went out in early March to collect them. Winter in Minnesota doesn’t really end until late April, so that was unfortunate but they still were exposed to some pretty cold temperatures as we had a polar vortex event in December 2019. In addition to our sites in Minnesota, we had a control group out in Delaware where our insects were reared that were left in an open-air insectary. This group was exposed to much warmer winter temperatures. In the following graph, you can see the temperatures we recorded above and below the snow at each site.\n\n\n\n\n\nTime series of the average ambient temperature recorded near bolts placed at three sites in Minnesota underneath the snow, in the air above the snow, as well as one site in an open air insectary in Delaware. Temperatures were recorded from 16 December 2019 - 25 March 2020 by a HOBO data logger.\n\n\n\n\nOnce we collected the logs, we took them back to the lab and threw them in incubators to let the insects develop and emerge. We had some concerns that the wood dried out over the winter, since it wasn’t part of a living tree, and this would make it difficult for any new adults to chew their way out from underneath. To address that concern, we let the logs sit for a few weeks to let any adults emerge that would emerge, then we scraped away the bark to look for prepupae that died over the winter and prepupae that had successfully developed to the adult form but were unable to emerge from the log. We fit two logistic regression models using the proportion that eclosed (fancy word for completed development) and the proportion that emerged from the logs as our response variable and temperature as the predictor. We originally included whether they were under snow or above the snow, but this term was not statistically significant so it was dropped from the model (not to mention, heavily correlated with temperature). These models are actually generalized estimating equations, which are one method for handling data that are correlated. These models allow you to specify a correlation structure for the data, to ensure that our variance estimates aren’t too narrow. Because each Spathius galinae lays multiple offspring on a single emerald ash borer (these groups of offspring are called a ‘brood’), we would expect the fate of each larvae in a brood to be correlated with the other larvae in the brood. So we include an exchangeable correlation structure in our variance to capture that structure in the data:\n\\[\nvar(Y_i) = \\sigma^2\\begin{bmatrix}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & ... & \\rho \\\\\n\\rho & \\rho & ... & \\rho \\\\\n\\rho & \\rho & ... & 1\\\\\n\\end{bmatrix}\n\\]\nAn exchangeable correlation structure lets us assume that all larvae within a brood are equally correlated and no one brood is more or less correlated than any other. Generalized estimating equations have the benefit of being marginal models, and so we’re marginalizing over the variation due to individual broods (as in, genetics) and estimating a population level response to cold.\n\n\n\nProportion of *Spathius galinae* eclosing or emerging after overwintering from 16 December 2019 -- 25 March 2020 at three locations in Minnesota and in Delaware. Larvae in Minnesota either overwintered in bolts underneath the snow or tied at breast height to a tree trunk and exposed to the ambient air. The sample size used in calculating each proportion is listed in parenthesis.LocationUnder SnowAirProportion EclosedProportion EmergedProportion EclosedProportion EmergedNewark, DE--0.6 (91)0.34 (91)Morris, MNa0.99 (199)0.62 (199)0.02 (49)0 (49)St. Paul, MN0.49 (87)0.3 (87)0.85 (48)0.38 (48)Waseca, MN0.72 (43)0.4 (43)0 (41)0 (41)aA small group of bolts that were shipped to Minnesota was placed immediately at 25°C (70% RH). Of the 177 larvae present in these segments, 98% eclosed and 72% emerged.\n\n\nIn the graphs below we can see how our insects fared. As expected, we see fewer insects emerging and developing the colder it got. It looks like about -30 °C is approaching the lower limit of survival for these insects. Interestingly, there were a few insects that were able to complete development at that low temperature (although they did not actually emerge from the logs). Given that those insects likely froze and still survived, this suggests that there may be variation in the population of wasps that allows for different strategies to tolerate the winter! Most of the insects appear to be chill tolerant (or freeze intolerant, as they die after freezing), but there may be some subset that is freeze tolerant. This is pretty exciting and a cool avenue for future work would be further testing to see if this is actually true.\n\n\n\n\n\nRelationship between the probability of eclosing (A) or emerging from underneath the bark (B) for Spathius galinae (n = 558) and the lowest temperature experienced over the winter of 2019 - 2020. Larvae were overwintering in bolts under the snow or tied to a treek trunk above the snowline in Minnesota, USA. Open points are the proportion of each brood that eclosed (A) or emerged (B) and have been jittered to facilitate viewing. Solid black points are the overall proportion of S. galinae eclosing (A) or emerging (B) at a given temperature, while error bars are ± one standard error. The solid line is the fitted population model. Grey bands represent 95% confidence interval for the fitted line.\n\n\n\n\nThe final part of this project was to use the models we developed to produce predictions of survival! I pulled minimum winter temperatures rasters for two winters: a relatively mild winter (2017-2018 winter) and a relatively severe winter, with a polar vortex event (2013-2014 winter). Then I fed those temperatures into the models we produced both from the lab and the field to predict how many insects would complete development (aka eclose) and how many would remain unfrozen. The predictions are restricted to the range of ash in North America, as you can’t have Spathius galinae without emerald ash borer and you can’t have emerald ash borer without ash.\n\n\n\n\n\nPredictions and confidence intervals of the percent of S. galinae that will eclose or initiate freezing based on minimum winter temperatures. Figures A - C display the lower 95% confidence interval prediction, predicted value, and the upper 95% confidence interval prediction, respectively, for a winter with relatively mild minimum temperaures (2017 - 2018), while figures D - F represent the same values for a winter with extreme minimum temperatures due to a polar vortex (2013 - 2014).\n\n\n\n\nWe used all three models because each measures a slightly different thing and it allows us to account for the experimental uncertainty inherent to this system. The models of eclosion produced from the lab are our most optimistic models, predicting high survival under even relatively cold temperatures. The field model for eclosion is slightly less optimistic, but we still see survival in even the coldest areas. The right-most column of maps shows us how many insects we would expect to freeze based on those measured exotherms, and even though we see 100% freezing in the far north, our results suggest some proportion of the insects should survive temperatures this cold.\nOf course, emerald ash borer is also an insect and is also adversely affected by these cold temperatures! Other work has shown that emerald ash borer at least as cold tolerant as Spathius galinae, and maybe a bit more. This means that even if Spathius galinae has a harder time in the colder areas, emerald ash borer will too. Releases of the wasp can be focused in areas that aren’t at the extreme edges and we should expect good performance across most of the northern range of ash in North America!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2022-01-12_you-are-what-you-eat/index.html",
    "href": "posts/2022-01-12_you-are-what-you-eat/index.html",
    "title": "Even caterpillars are what they eat",
    "section": "",
    "text": "For my Master’s, I spent far too much time working with an invasive caterpillar Lymantria dispar (this caterpillar may be familiar to those of you from the eastern US under a different common name, but in 2021 the common name has been dropped due to the negative connotations it carried for some eastern European peoples. I won’t be using it except when it’s [unfortunately] included in a citation). I reared a bunch of these little guys in the lab for my experiments and it was such a pain.\n\n\n\nA Lymantria dispar caterpillar. The caterpillars are quite pretty (the adults are just boring brown/white moths), but I wouldn’t pick them up. Those hairs are what we call ‘urticating hairs’ and they can be quite irritating. They’re not deadly like some caterpillars, but they may cause a skin reaction or worse if you’re allergic. Photo from Washington State Department of Agriculture.\n\n\nFirst, they cannibalize each other even when food is available. Second, they are extremely susceptible to some kind of mold when grown in the lab. No amount of sanitation (of myself, the housing, the food) seemed to help contain it. I would have to rear about 500 at a time if I wanted 100 to use in experiments. And they smelled awful! By the time I was done with my thesis, I never wanted to look at another L. dispar caterpillar. Since then though, I’ve come around on them. They’re a unique insect in the world of invasive species because we know when and where they were introduced, and they’ve been studied to death as a model invasive insect. You can read all about the introduction of this insect in the 1896 tome “The Gypsy Moth Porthetria dispar” (the genus of this species has since changed). I’ll tell you some of the more interesting bits here though.\n\n\n\nTwo L. dispar caterpillars eating some of the artifical diet we feed them, and their poop pellets.\n\n\nEtienne Trouvelot brought these insects over from France (L. dispar is native to Europe and Asia) to Medford, MA around 1869. Etienne was a naturalist, astronomer, and artist and at the time, was interested in producing a more hearty breed of silk worm. You see, silks worms are pretty but they’re very picky and incredibly hard to rear. Lymantria dispar, on the other hand, will eat just about anything you give it and (despite my experiences with it), much easier to rear while also producing their own silk! Etienne was hoping to cross-breed these two caterpillars. He apparently was not aware that they’re two very different species and cross-breeding would not be successful. Regardless, one day the L. dispar caterpillars he was keeping got loose! This part of the story is less certain - was it a thunderstorm that knocked open the enclosures? Did they escape some other way? but no matter what they got out.\n\n\n\nOne of Etienne Trouvelot’s astronomy paintings. His artwork is quite nice - a friend of mine got me several prints of his work as a gift when I finished my Masters\n\n\nTo Etienne’s credit, he immediately tried to recover them. He tried enlisting members of the town to help him, but at the time the concept of an invasive species didn’t really exist and no one bothered to help. Within the next decade, their populations grew out of control! There are quotes in the aforementioned 1896 publication of people gathering them by the bucket full in the morning, finding them in their closets, or being able to hear their frass (poop) raining down from trees on the roofs of their house. At this point, it was too late to control them and their populations began to grow and spread. Their egg masses are small and brown and easily hidden on firewood or on the underside of cars (or that far back in time, carriages), so they’re often inadvertently transported around which contributes to their spread.\nThese insects are prolific feeders and they feed on virtually all species of deciduous trees in the eastern US and even a few species of conifers. The extensive defoliation they cause during outbreaks won’t often kill trees, but repeated defoliation year after year will weaken trees and expose them to secondary mortality agents. They’re also gross when they outbreak - they get EVERYWHERE. And as the hairs on their bodies can be irritating to humans and pets, so they’re a bit of a health hazard too. Plus, it’s not fun having trees without any leaves in your neighborhood during outbreaks.\n\n\n\nA bucket of L. dispar caterpillars collected in Indiana. Photo from Purdue Entomology Extension\n\n\nThe US federal government has maintained a quarantine for this insect and has regulations in place to limit it’s movement because of the costs associated with it’s defoliation. Without getting into the weeds, my Master’s research focused on evaluating the movement potential and behavior of these caterpillars because we needed to understand how far they could crawl to ensure regulatory practices were appropriate, which brings me to the topic of this post.\nFor one of my thesis chapters, I studied the effect of different food sources on the movement of L. dispar caterpillars. One regulatory practice in the wood processing industry required that wood be stored at least 100 feet away from any standing trees, on the assumption that this would be far enough that caterpillars and adults wouldn’t find their way to the wood and lay eggs on it. However, there’s very little research (i.e. none) that supported this number. We wanted to get some baseline data on the movement of the caterpillars and their behavior. The type and quantity of food can have a large impact on the movement capabilities and behavior of insects. Given how many different types of plants L. dispar has been shown to eat, we wanted to see if preferred hosts and non-preferred hosts caused different movement behaviors.\n\n\n\nIn the summer of 2016, parts of New England had a significant L. dispar outbreak. This image from the Nasa Earth Observatory picks up the defoliation damage.\n\n\nThese experiments were all conducted in the lab to give us a real controlled environment. Now, you may be wondering, how did we study the movement of an insect in a lab based environment? We used a servosphere! This futuristic sounding device is a ball situated atop three orthogonal motors. A camera pointed at the top of the ball watches an insect as it crawls around on top and it signals to the motors to rotate the ball so that the insect is kept in position at the apex of the ball. Essentially, it’s a treadmill but for insects!\n\n\n\nOne of our cockroach pets demonstrating the servosphere\n\n\nWe fed our caterpillars 5 different foods: Bur oak, eastern larch, Norway maple, silver maple, and the artificial diet commonly used to raise them in the lab. We chose these foods as they cover a range of what are considered preferred, with bur oak and eastern larch being listed as preferred, Norway maple being somewhat preferred, and silver maple rarely being attacked. Because these insects are used often in lab studies, we also wanted to know how the artificial diet may be affecting their behavior. The caterpillars were further split into three feeding regimes where they were starved for 0, 24, or 48 hours prior to being placed on the servosphere. We know food deprivation can increase the movement of other insects and we wanted to understand how their movement might change once they’ve consumed all the food in an area.\nSo we but a bunch of caterpillars on the servosphere and recorded their movement for 10 minutes (after a 5 minute acclimation period). Somewhat surprisingly, when the larvae were not starved, they tended not to move. (You’ll notice silver maple is missing from these graphs - the caterpillars refused to eat it and all died).The longer they had gone without food though, the more likely they were to move. Except larch. Those fed larch were moving all the time.\n\n\n\nThe probability that a caterpillar would move more than 10 centimeters. Any caterpillar that didn’t move more than 10 centimeters during the 10 minute observation period was considered a non-mover. Lines are the fitted values for a logistic regression model with food deprivation period as the input variable. Dashed lines represent non-significant slopes, while shaded grey areas are the 95% confidence intervals.\n\n\nThe caterpillars fed larch always moved, but they actually moved less distance the longer they were starved. In contrast, the larvae we fed on oak increased how far they moved (and there was more variance in how far they moved), as did those fed on artificial diet.\n\n\n\nThe total distance moved by caterpillars deprvied of food for different periods of time.\n\n\nSo, we feel that our results show that larvae feeding on what are considered preferred hosts tend to move further as starvation increases. One possible explanation is that these hosts (bur oak) are preferred for a reason: they’re more nutritious/the insects can access more of the nutrition. So when they are eating it (not starved) they’re content and just hang out. When they have eaten it and then are deprived of it, they’re in a better state physiologically to equip them to move further.\nThe results from the caterpillars fed on larch stood out to us. It is listed as a susceptible (i.e. preferred) host in the literature, which makes it unique among conifers. Most conifers are not susceptible at all to European L. dispar (Asian L. dispar is another story, and it’s own subspecies). While the larvae did eat the larch, it seems to change their movement behavior, especially compared to bur oak. This is likely also tied back to the nutrition of the tree itself and suggests that, while L. dispar will eat it, it’s definitely not the best food for them.\nSo, this study didn’t give us any groundbreaking discoveries but it gave us a starting place with which we could evalute some of the regulatory practices (the focus of my other chapter).\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2022-01-19_wow-ah-pt1/index.html",
    "href": "posts/2022-01-19_wow-ah-pt1/index.html",
    "title": "Download data from the World of Warcraft auction house REDUX",
    "section": "",
    "text": "The first of my scripts pulls the auction house data from the Blizzard API hourly. I use a config file to store my API keys for both the dropbox API and Blizzard API. I wrote a couple functions to pull the data from the Blizzard API using information specific to the server I play on. You could repurpose this code for your own server, but you’d need to update the id variable for your server.\n\n#########\n# This script uses access tokens for dropbox and Blizzard API keys to scrape\n# data from the WoW auction house and upload the csv to drop box.\n# The pull_data.py script will pull the data from dropbox, delete it once pulled\n# and add it to the database. \n#########\nimport requests\nimport os\nfrom datetime import datetime\nimport pandas as pd\nimport dropbox\nfrom decouple import config\n\nDROPBOX_ACCESS = config('DROPBOX_ACCESS')\ndbx = dropbox.Dropbox(DROPBOX_ACCESS)\n\n# Create access toeken\ndef create_access_token(client_id, client_secret, region = \"us\"):\n   data = { 'grant_type': 'client_credentials' }\n   response = requests.post('https://%s.battle.net/oauth/token' % region,\n                            data = data,\n                            auth = (client_id, client_secret))\n   return response.json()\n\n# Get Malf and connected realms data\ndef get_malfurion(search):\n   search = search\n   response = requests.get(search)\n   return response.json()[\"auctions\"]\n\nCLIENT_ID_WOW = config('CLIENT_ID_WOW')\nCLIENT_SECRET_WOW = config('CLIENT_SECRET_WOW')\nresponse = create_access_token(CLIENT_ID_WOW, CLIENT_SECRET_WOW)\ntoken = response['access_token']\n\n# ID for NA-Malfurion server\nid = 1175\nsearch = f\"https://us.api.blizzard.com/data/wow/connected-realm/{id}/auctions?namespace=dynamic-us&locale=en_US&access_token={token}\"\n\n# Get Data\nauction_data = get_malfurion(search)\n\nNext steps are to clean the data. We take the JSON file and turn it into a Pandas dataframe. I rename some of the columns, expand others, and fill in NA values. There are two variables that represent how much an auction is posted for: buyout and unit_price. In game, some items are “stackable”, like potions. You can stack 20 of them in one inventory spot. Other items are not stackable, like armor. A pair of boots occupies a single inventory space and cannot stack. The buyout price is what is used for unstackable items, like boots, and the unit_price is the cost of a stackable item. The distinction isn’t really important for my purposes, so I combine them into a single cost column and then convert that to cost in gold. I also add some datetime columns for future use in modeling. I could do this more efficiently and just have a single datetime column from which I could derive the others, but right now storage space isn’t a concern. At the end of the cleaning, I write the dataframe to a csv file.\n\n# Convert data to a dataframe\nauction_data = pd.DataFrame(auction_data)\n\nauction_data = auction_data.rename(columns={\"id\": \"auction_id\",})\n# Expand the item column\nauction_data = pd.concat([auction_data.drop(['item'], axis=1), auction_data['item'].apply(pd.Series)], axis=1)\nauction_data['id'] = auction_data['id'].map(int).map(str) # Convert to int to remove trailing 0, then to string\n# Unit prices are for stackable items, buyout is for unstackable. I just want a single gold cost column\nauction_data['buyout'] = auction_data['buyout'].fillna(0)\nauction_data['unit_price'] = auction_data['unit_price'].fillna(0)\nauction_data['cost'] = auction_data['buyout'] + auction_data['unit_price']\n# Cost is in copper, convert to gold\nauction_data['cost_g'] = auction_data['cost'] / 10000\n# Remove rows for pets. I don't care about pets\n# And remove columns about pets\nis_pet_mask = auction_data['pet_breed_id'].isna()\nauction_data = auction_data[is_pet_mask]\nauction_data.drop(columns = ['pet_breed_id', 'pet_level', 'pet_quality_id', 'pet_species_id'], inplace = True)\n\n#   These are subgroups of an equipable item with the bonus stats (intellect agility, strength, etc)\n# Make one date time column and also 4 columns for each relevant piece\nauction_data['date_time'] = datetime.now()\nauction_data['collection_year'] = datetime.now().strftime('%Y')\nauction_data['collection_month'] = datetime.now().strftime('%m')\nauction_data['collection_day'] = datetime.now().strftime('%d')\nauction_data['collection_hour'] = datetime.now().strftime('%H')\nfilename = datetime.now().strftime('Malfurion_NA-%Y-%m-%d-%H-%M.csv')\n\nHere, I upload the csv file to Dropbox and delete the csv file locally. I do this because eventually I want to migrate this script to something like a Raspberry Pi so I can have it constantly run. Right now it’s running constantly on my personal PC, and I don’t want to do that forever. I want to be able to turn on my PC and have it scrape what’s been collected in Dropbox since the last time. I tried automating this with Github Actions, but found that Github Actions would not run every hour (at least, not for a free plan). You can ask it to run hourly, but depending on demand it may or may not. I was getting some scrapes occurring within 20 minutes of each other and others hours apart, which isn’t useful for an API that is updated hourly.\n\ndropbox_filename = \"/\" + filename\nauction_data.to_csv(filename, index = False)\n\nwith open(filename, 'rb') as f:\n   dbx.files_upload(f.read(), dropbox_filename)\n\nos.remove(filename)\n\nI use Windows Task Scheduler to run the above code ever hour on the hour. Then I have a script that pulls the data from Dropbox and places it into a SQLite database, which I’ll detail below. At this point, it’s overkill (why repull the data I already had?) but like I said earlier, I eventually want to migrate the first script to a different computer.\nAgain, I need the config file to get the Dropbox API keys. Figuring out how to download from Dropbox was a bit of a pain in the ass. The documentation isn’t great for a beginner, but after some extensive googling I think I figured it out.\n\n############\n# This script pulls WoW AH data from my dropbox to my machine, where\n# I'm building a database of this data. \n############\nimport os\nfrom datetime import datetime\nimport pandas as pd\nimport dropbox\nfrom decouple import config\nimport pandas\nimport sqlite3\nDROPBOX_ACCESS = config('DROPBOX_ACCESS')\ndbx = dropbox.Dropbox(DROPBOX_ACCESS)\n# Had some issues with this script. The only workaround I could find\n# was to specify the full directory path.\nmain_dir = 'F:/Documents/WoWAH_py/WoWAH_python/'\n# Download csvs from dropbox\n# These go in the temp_csvs folder until they're added to the databse\n# then they're deleted\ndb_files = dbx.files_list_folder(\"\")\nfor i in db_files.entries:\n    with open(main_dir + 'data/temp_csvs/'+ i.name, \"wb\") as f:\n        metadata, res = dbx.files_download(i.path_lower)\n        f.write(res.content)\n\nNow, I connect to the local database and append the hourly data to the existing auctions table. This should also work if the auctions table doesn’t exist already; it’ll generate it the first time this is run. Then I delete the csv files from Dropbox, since I have a free plan and don’t have unlimited space.\n\n# Connect to database and add csvs\nconn = sqlite3.connect(main_dir + 'data/WoWAH_db.sqlite')\n\nah_csvs = os.listdir(main_dir + 'data/temp_csvs')\ncurs = conn.cursor() # Create cursor w/e that is\n\nfor i in ah_csvs:\n    temp_file = pandas.read_csv(main_dir + 'data/temp_csvs/' + i)\n    temp_file['id'] = temp_file['id'].map(str)\n    temp_file['date_time'] = pd.to_datetime(temp_file['date_time'])\n    temp_file.to_sql('auctions', conn, if_exists='append', index = False)\n\n# Once files are in the database, go ahead and delete from dropbox\nfor i in db_files.entries:\n    dbx.files_delete(\"/\" + i.name)\n\n# And delete  csvs\nfor i in ah_csvs:\n    os.remove(main_dir + 'data/temp_csvs/' + i)\n\nThis last step queries the database to get the most recent 4 weeks of data and writes it to a csv, which is used locally by a Shiny app I’ve built. One thing to note is that when I pull the auction data, I don’t get the name of the items up for auction. Instead the items all have an id number. I have to use that number in an additional API call to get the item name and other information about the items. So in a later post, I’ll detail how I use unique item ids from the auctions table to build an item table in the database, update that table as new, not-seen-before items show up in the auctions table. You can see that I use a left join to bring over relevant columns from the item_id table.\nYou can find a demo of this app here. The demo uses a subset of old data, since I’m limited to a free plan with only so much storage. Eventually I’ll update it to deploy a subset of new data ever hour, but that’s for a later time!\n\n# Write a csv to file that contains the most recent 2 (4?) weeks of data\nquery = curs.execute(\"\"\"\n    SELECT auctions.auction_id,\n        auctions.quantity,\n        auctions.time_left,\n        auctions.date_time,\n        auctions.cost_g,\n        item_id.name,\n        item_id.is_stackable,\n        item_id.is_equippable\n    FROM auctions\n    LEFT JOIN item_id ON auctions.id = item_id.id\n    WHERE date_time BETWEEN datetime('now', '-1 month') and datetime('now', 'localtime')\n    \"\"\")\ncols = [column[0] for column in query.description]\nresults = pd.DataFrame.from_records(data = query.fetchall(), columns = cols)\nresults.to_csv(main_dir + '/data/latest_month_auctions.csv', index = False)\n# Write a smaller version of the data to upload with the app as a test\nquery = curs.execute(\"\"\"\n    SELECT auctions.auction_id,\n        auctions.quantity,\n        auctions.time_left,\n        auctions.date_time,\n        auctions.cost_g,\n        item_id.name,\n        item_id.is_stackable,\n        item_id.is_equippable\n    FROM auctions\n    LEFT JOIN item_id ON auctions.id = item_id.id\n    WHERE date_time BETWEEN datetime('now', '-1 month') and datetime('now', 'localtime')\n    AND name IN ('Rising Glory',\n            'Widowbloom',\n            'Marrowroot',\n            \"Vigil's Torch\",\n            'Death Blossom',\n            'Nightshade',\n            'Laestrite Ore',\n            'Elethium Ore',\n            'Solenium Ore',\n            'Oxxein Ore',\n            'Phaedrum Ore',\n            'Sinvyr Ore',\n            'Angerseye',\n            'Oriblase',\n            'Umbryl',\n            'Desolate Leather',\n            'Callous Hide',\n            'Pallid Bone',\n            'Gaunt Sinew',\n            'Heavy Desolate Leather',\n            'Heavy Callous Hide',\n            'Shrouded Cloth',\n            'Lightless Silk',\n            'Soul Dust',\n            'Sacred Shard',\n            'Eternal Crystal')\n    \"\"\")\ncols = [column[0] for column in query.description]\nresults = pd.DataFrame.from_records(data = query.fetchall(), columns = cols)\nresults.to_csv(main_dir + 'wowah_app/app_data/latest_month_auctions.csv', index = False)\n\nconn.close()\n\nSo this details my automated workflow to pull WoW AH data with Python. It’s set up so I can eventually migrate it to a different computer, so it’s got some redundancies in how it functions right now.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/2022-08-19_generative_art_1/index.html",
    "href": "posts/2022-08-19_generative_art_1/index.html",
    "title": "Generative Art! Part 1",
    "section": "",
    "text": "I’ve been following Danielle Navarro and Thomas Lin Pederson on Twitter for some time. These two introduced me to generative art, and I kind of love the art they make. I love the idea of taking data and making artwork from it, through transformations or choices in how to display or map the data to the visualization. When I found out Danielle had made her workshop available as a webpage, I was real excited to work through it. So, here I am! I’ll be reproducing some of her code here just for my own edification, and also working through some of the exercises she provides. This post will just be working through her very first section on technique and color. I’ll tackle the others later.\nFirst, let’s take just a few minutes and try to make some art out of mpg like directed.\nOkay! It’s a thing. Not my favorite thing, but it’s a thing.\nNow on to using the polar art function she provided.\nNow I’m supposed to try editing the polar art to create my own system.\nThis new function drops the coord_polar, so I probably should rename it from polar_art, but I’m feeling a bit lazy. I might call this one “Windmill Art”. I kind of like how using a normal distribution for the ends causes some “outliers” to appear that are much longer than the others."
  },
  {
    "objectID": "posts/2022-08-19_generative_art_1/index.html#colors",
    "href": "posts/2022-08-19_generative_art_1/index.html#colors",
    "title": "Generative Art! Part 1",
    "section": "Colors",
    "text": "Colors\nThe coolors.co website that Danielle introduces us to seems really awesome for generating custom color palettes.\n\nlibrary(scales)\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.0.5\n\nshow_col(c(\"#001021\",\"#034748\",\"#1481ba\",\"#11b5e4\",\"#0caadc\"))\n\n\n\n\nThe scale_color_gradientn() function will generate a sequence of colors interpolated between your chosen palette, or it can be done manually using colorRampPalette(). Note that colorRampPalette() actually returns a function that takes an integer to specify how many colors to return.\n\ncolorRampPalette(c(\"#001021\",\"#034748\",\"#1481ba\",\"#11b5e4\",\"#0caadc\"))(100)\n\n  [1] \"#001021\" \"#001222\" \"#001424\" \"#001625\" \"#001827\" \"#001B28\" \"#001D2A\"\n  [8] \"#001F2C\" \"#00212D\" \"#01242F\" \"#012630\" \"#012832\" \"#012A33\" \"#012C35\"\n [15] \"#012F37\" \"#013138\" \"#01333A\" \"#02353B\" \"#02383D\" \"#023A3E\" \"#023C40\"\n [22] \"#023E42\" \"#024043\" \"#024345\" \"#024546\" \"#034749\" \"#03494D\" \"#044C52\"\n [29] \"#054E56\" \"#05505B\" \"#065360\" \"#075564\" \"#075769\" \"#085A6E\" \"#095C72\"\n [36] \"#0A5F77\" \"#0A617B\" \"#0B6380\" \"#0C6685\" \"#0C6889\" \"#0D6A8E\" \"#0E6D92\"\n [43] \"#0E6F97\" \"#0F719C\" \"#1074A0\" \"#1076A5\" \"#1178A9\" \"#127BAE\" \"#127DB3\"\n [50] \"#137FB7\" \"#1382BA\" \"#1384BC\" \"#1386BE\" \"#1388BF\" \"#138AC1\" \"#138CC3\"\n [57] \"#138EC5\" \"#1390C6\" \"#1292C8\" \"#1294CA\" \"#1297CB\" \"#1299CD\" \"#129BCF\"\n [64] \"#129DD0\" \"#129FD2\" \"#12A1D4\" \"#12A3D6\" \"#11A5D7\" \"#11A7D9\" \"#11A9DB\"\n [71] \"#11ACDC\" \"#11AEDE\" \"#11B0E0\" \"#11B2E1\" \"#11B4E3\" \"#10B4E3\" \"#10B4E3\"\n [78] \"#10B3E3\" \"#10B3E2\" \"#10B2E2\" \"#0FB2E2\" \"#0FB2E1\" \"#0FB1E1\" \"#0FB1E1\"\n [85] \"#0FB0E0\" \"#0EB0E0\" \"#0EAFE0\" \"#0EAFDF\" \"#0EAEDF\" \"#0EAEDF\" \"#0DAEDE\"\n [92] \"#0DADDE\" \"#0DADDE\" \"#0DACDD\" \"#0DACDD\" \"#0CABDD\" \"#0CABDC\" \"#0CAADC\"\n [99] \"#0CAADC\" \"#0CAADC\"\n\nshow_col(colorRampPalette(c(\"#001021\",\"#034748\",\"#1481ba\",\"#11b5e4\",\"#0caadc\"))(100))\n\n\n\n\nTo visualize the colors as a smooth palette:\n\nimage(\n  x = matrix(1:100, ncol = 1), \n  col = colorRampPalette(c(\"#001021\",\"#034748\",\"#1481ba\",\"#11b5e4\",\"#0caadc\"))(50),\n  useRaster = TRUE,\n  axes = FALSE\n)\n\n\n\n\nCan get other palettes from canva_palettes from ggthemes\n\nshow_col(canva_palettes[[23]])\n\n\n\n\nCould write a function that randomly samples a palette from these palettes\n\nsample_canva <- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\n\npolar_art(seed = 2, n = 100, palette = sample_canva(seed = 120)) +\npolar_art(seed = 2, n = 100, palette = sample_canva(seed = 82)) +\npolar_art(seed = 2, n = 100, palette = sample_canva(seed = 44))\n\n\n\n\nOr if I want to use the same palette but vary the art\n\npolar_art(seed = 26123, n = 100, palette = sample_canva(seed = 82)) +\npolar_art(seed = 243, n = 100, palette = sample_canva(seed = 82)) +\npolar_art(seed = 12342, n = 100, palette = sample_canva(seed = 82))\n\n\n\n\nNow, time to try creating my own function to sample random colors to build a completely random palette.\n\nsample_named_colours <- function(n) {\n   sample(colours(distinct = TRUE), n)\n}\n\n\npolar_art(seed = 29, n = 50, palette = sample_named_colours(8))\n\n\n\n\n\n# This function should randomly sample from the 600 distinct colors available\n# from canva\nrandom_sample_canva <- function(n) {\n   sample(unlist(canva_palettes), n)\n}\n\n\npolar_art(seed = 29, n = 500, palette = random_sample_canva(120))\n\n\n\n\nThe polar art function is nice, but it doesn’t give a lot of control. Danielle provides some code to separate out the data generation from the plotting, and also sets up the plot function so we can alter the geom we are plotting.\n\nsample_data <- function(seed = NULL, n = 100){\n  if(!is.null(seed)) set.seed(seed)\n  dat <- tibble(\n    x0 = runif(n),\n    y0 = runif(n),\n    x1 = x0 + runif(n, min = -.2, max = .2),\n    y1 = y0 + runif(n, min = -.2, max = .2),\n    shade = runif(n), \n    size = runif(n),\n    shape = factor(sample(0:22, size = n, replace = TRUE))\n  )\n}\npolar_styled_plot <- function(data = NULL, palette) {\n  ggplot(\n    data = data,\n    mapping = aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      size = size\n    )) + \n    coord_polar(clip = \"off\") +\n    scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1), \n      oob = scales::oob_keep\n    ) +\n    scale_x_continuous(\n      expand = c(0, 0), \n      limits = c(0, 1), \n      oob = scales::oob_keep\n    ) + \n    scale_colour_gradientn(colours = palette) + \n    scale_size(range = c(0, 10)) + \n    theme_void() + \n    guides(\n      colour = guide_none(),\n      size = guide_none(),\n      fill = guide_none(),\n      shape = guide_none()\n    )\n}\n\n\ndat <- sample_data(n = 100, seed = 1231) \npal <- sample_canva(seed = 63)\n\npolar_styled_plot(data = dat, palette = pal) + geom_segment() +\npolar_styled_plot(data = dat, palette = pal) + geom_path() +\npolar_styled_plot(data = dat, palette = pal) + geom_point()\n\n\n\n\nThis style of function writing also allows us to construct pieces that combine multiple geoms and/or data\n\nlibrary(dplyr)\n\ndat1 <- sample_data(n = 2000, seed = 1223) \ndat2 <- sample_data(n = 100, seed = 4536)  %>%   \n  mutate(y0 = .3 + y0 * .6, y1 = .3)\n\npolar_styled_plot(palette = sample_canva(seed = 7)) + \n  geom_segment(\n    data = dat1  %>%  mutate(size = size * 3)\n  ) + \n  geom_segment(\n    data = dat2 %>% mutate(size = size / 5), \n    lineend = \"round\", \n    colour = \"white\"\n  ) +\n  geom_segment(\n    data = dat2 %>% mutate(size = size / 40), \n    lineend = \"round\", \n    colour = \"#222222\"\n  ) +\n  geom_point(\n    data = dat2 %>% mutate(size = size * 2),\n    colour = \"#222222\"\n  )\n\n\n\n\nAlright, now time to try this on my own. For this next exercise I’m supposed to use the sample_data(), polar_styled_plot() and adding different geoms to make my own pieces.\n\ndat1 <- sample_data(n = 50, seed = 54267)\ndat2 <- sample_data(n = 20, seed = 123)\ndat3 <- sample_data(n = 100, seed = 54573)\n\n# I think I'll use this one as the preview image for the blog\npolar_styled_plot(palette = sample_canva(seed = 10928)) +\n   geom_point(data = dat3, aes(size = size)) +\n   geom_line(data = dat1, aes(size = size / 20))\n\n\n\nggsave('posts/2022-08-19_generative_art_1/preview.jpg')\n\nSaving 7 x 5 in image\n\npolar_styled_plot(palette = sample_canva(seed = 4346)) +\n   geom_point(data = dat2, size = 6) +\n   geom_line(data = dat2, size = 0.5) +\n   geom_point(data = dat2%>% mutate(y1 = y1 - 0.8, x0 = x1 - 0.2), \n              size = 6) +\n   geom_line(data = dat2 %>% mutate(y1 = y1 - 0.8, x0 = x1 - 0.2),\n             size = 0.5) +\n   geom_point(data = dat2%>% mutate(y1 = y1 + 0.4, x0 = x0 - 0.2), \n              size = 6) +\n   geom_line(data = dat2 %>% mutate(y1 = y1 + 0.4, x0 = x0 - 0.2),\n             size = 0.5)\n\n\n\n\nI think I like the first of these two better. It’s kind of got an aggressive toddler connecting the dots vibe that I like. The other one feels a bit kitschy to me. This is lots of fun! I have almost no other artistic talents, so this is a cool way to express myself and try to find aesthestics that I enjoy. I look forward to working through more of these tutorials as time allows."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Please reach out to me if you are interested in copies of my academic papers. My contact information is available on my CV."
  },
  {
    "objectID": "projects.html#phd-projects",
    "href": "projects.html#phd-projects",
    "title": "Projects",
    "section": "PhD projects",
    "text": "PhD projects\nAfter finishing my MSc, I stuck around at the University of Minnesota to complete a PhD in Forest Entomology with a graduate minor in Biostatistics. My PhD has focused on answering questions pertaining to the management of emerald ash borer, an invasive wood boring beetle in North America that is killing all of our ash trees.\nMy first chapter focused on evaluating the cold tolerance of a parasitic wasp, Spathius galinae, that we use to control populations of emerald ash borer. Here’s a link to the paper. Code and data associated with this project are available on the Data Repository of the University of Minnesta.\nMy second chapter evaluated the attraction radius of a sex pheromone we use in traps for monitoring emerald ash borer populations. Link to the paper is here and to the repository with code and data here.\nMy third chapter focuses on identifying environmental factors that have contributed to variable spread rates of emerald ash borer. This is not yet published, so there is no paper or code available. I’ll update it once it is!"
  },
  {
    "objectID": "projects.html#masters-projects",
    "href": "projects.html#masters-projects",
    "title": "Projects",
    "section": "Master’s projects",
    "text": "Master’s projects\nI did my MSc at the University of Minnesota in the Forest Entomology lab. The focus on my research was understanding the behavior of the invasive moth Lymantria dispar (formerly known as the gypsy moth, but this common name has been retired due to the negative connotations associated with the word “gypsy”). The code associated with my two thesis chapters can be found on my GitHub here and here. There are also plain language summaries in the README for each project, as well as links to the paper. Please reach out if you would like a copy of either of these papers. Be warned if you look at the code: I was brand new to R and programming in general! It is probably kind of a mess."
  },
  {
    "objectID": "projects.html#hobby-projects",
    "href": "projects.html#hobby-projects",
    "title": "Projects",
    "section": "Hobby projects",
    "text": "Hobby projects\n\nRunnr\nI love to run. I came to it a little bit later in life than some - it wasn’t until my senior of college that I began to run with any consistency. Now, almost exactly 10 years after I started running regularly, I’ve run several marathons and ultramarathons. One thing I love about running is the potential to collect lots of data about the activity! I started keeping a running log 10 years ago. I would track my mileage (using Google Earth to determine how far I went before I owned a GPS watch) and times. I’ve since expanded my data to record my average heart rate, the type of run it was, and the shoes I ran in. I created the Runnr Shiny app as a way to visualize some of that data. Like all my hobby projects, it isn’t very polished but it does what I want. I use it to get a general gauge of how my training is going, track the mileage on my shoes to know when I need to replace them, and just reflect on the miles I’ve put in over the years!\n\n\nScraping the World of Warcraft auction house\nIn my free time, I enjoy playing a little World of Warcraft. In this game, there is an auction house where players can buy and sell in-game items with in-game currency. There is a lot of in-game gold to be made by playing the auction house (and even whole communities that focus mainly on making gold as their main form of playing the game). The game publisher, Blizzard, has an API set up for people to download data on all the auctions on a given server’s auction house available every hour. While other websites collate this data for economically minded players to take advantage of, I wanted to be able to explore the data more in-depth so I set about learning how to use the API. The code for this project is on my GitHub. World of Warcraft uses a subscription model, so it costs me $15/month to play but Blizzard makes it possible to use in-game gold to buy game time. I use this data to help my gold making operations so I don’t have to pay real money to play this game.\nThis is currently a work in progress. Right now, I use a combination of R and Python to scrape and munge the data from the API and then write it to a local database. I also have set up email alerts for certain items I’m interested in so I can be notified when they drop below a certain price. Eventually I want to set this up to run automatically using something like GitHub Actions so I don’t have to leave my personal PC on all the time. I also would like to set up a Shiny app for visualizing pricing data. I’m also in the process of using this data to learn more about time series analysis, so you’ll see some .Rmd files in this repository where I’m working through code from Hyndman and Athanasopulos’ “Forecasting: Principles and Practice”. Check out the blog portion of the website for some posts describing this project in more detail!"
  }
]