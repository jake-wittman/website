{
  "hash": "04c32d60855a5d42410473042ec8dc18",
  "result": {
    "markdown": "---\ntitle: \"Downloading data from the World of Warcraft Auction House\" # <---- UPDATE ME\ndescription:\n  How I leveraged my programming knowledge to avoid paying real money for a video game # <---- UPDATE ME\nauthor:\n  - first_name: \"Jake\"\n    last_name: \"Wittman\"\n    url: https://wittja01.github.io/website/\ndate: 2021-12-03\nimage: org_ah.jpg  # <---- UPDATE ME \ncitation_url: https://wittja01.github.io/website/time-is-money-friend \nrepository_url: https://github.com/wittja01/WoWAH\nparams:\n  slug: time-is-money-friend\n  date: 2021-12-03\n  repo: wittja01/WoWAH\n  site: https://wittja01.github.io/website\n---\n\n\n<!----\n\nchecklist:\n  - check the \"update me\" messages in YAML above\n  - initialise the _renv folder with refinery::renv_new(\"name of post folder\")\n  - populate the lockfile with refinery::renv_snapshot(\"name of post folder\")\n  - update the _renv folder from snapshot with refinery::restore(\"name of post folder\")\n\n---->\n\n\n<!--------------- setup post ----------------->\n\n\n\n\n\n\n<!--------------- post ----------------->\n\nIn my free time, I enjoy playing a little World of Warcraft. Some of my earliest childhood memories are sitting next to my dad while he would play RTS games like Warcraft 2 and Command & Conquer, and when I got a little bit older I started playing myself. Warcraft was always a favorite - I spent weekends in middle school with my friend's and we'd play Warcraft 3 late into the night. In high school we moved on to World of Warcraft. I stopped playing sometime toward the end of the Wrath of the Lich King expansion, but started again during the Covid-19 pandemic (so about a 10 year break!).\n\nUpon my return, I found the game had changed quite a bit! One piece that was new to me, and that I found instantly intriguing, was that you could now buy game time using in game gold! For the uninitiated, World of Warcraft is a subscription based game - you have to pay $15/month to play (in the USA anyway, prices are different in different locations). Being able to buy game time with in-game currency was a game changer for me. One of the reasons I quit playing WoW 10 years ago was because I didn't feel like I could justify paying 15 dollars a month when I was a poor undergrad. Now that I'm an only-very-slightly-less poor grad student, 15 dollars a month is still a fair bit of change but if I could pay for it with gold I earned while playing a game I love, that's great! As the goblin characters in WoW will tell you: \"Time is money, friend!\".\n\nI immediately got to work learning about how others were successful in translating their play time into game time. A lot of players will make their in-game gold playing the Auction House. In WoW, the Auction House is a place where players can (surprise!) auction off in-game items in exchange for currency. There are a variety of professions that players can choose from to gather or produce goods that are useful for other players. Blizzard (the game developer) also happens to have an API that allows you to access the data for all the auctions on a realm's Auction House, and this API is updated every hour. Services exist that tap into this API to help understand the price trends of items for the economically-minded player, but I wanted to level-up my programming skills a bit and build myself a tool that did exactly what I needed. So, I set about learning how to access this API, built a database of auctions, and set up some email alerts for items I was watching so I could buy low and sell high! Since I built this program, I've earned about 8 million in-game gold, which translates to roughly $600 worth of 'Blizzard Balance' (roughly because the gold cost of a 15 dollar token fluctuates over time). That's more than enough money for me to pay for my game time, plus more for me to buy time for friends or spend it on other Blizzard games.\n\nHere I'll walk through the code I used to do this. I wrote the script in an R markdown file so I could weave together R code chunks with Python code chunks. I used both languages in part because I want to learn more Python, so I thought why not, but also because I was having trouble accessing the API using R. Setting it up in Python worked almost flawlessly. I'm a total novice when it comes to scraping APIs, so I'm sure there was something I was doing wrong in R. You'll also see that I use renv to help manage both the R and Python environments - something else I'm learning more about! I've already run into dependency hell with a few R projects, so I'm very thankful for the `renv` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.time()\n\n# R imports\n.libPaths(\"path/to/your/project/renv\")\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(googledrive)\nlibrary(reticulate)\nlibrary(lubridate)\nlibrary(emayili)\nuse_condaenv(\"path/to/your/conda/env\")\n\n`%!in%` <- Negate(`%in%`)\n\n# I keep my API keys in this file (make sure you add this file to a .gitignore if you're using git!)\nsource('data/keys.R')\n```\n:::\n\n\nHere I set up the Python functions I'll need to get to the API. If you want to do this, you'll need to make sure you have your region specified.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Set up python functions\nimport requests\nfrom datetime import datetime\nimport pandas as pd\n\n# Create a new access token\ndef create_access_token(client_id, client_secret, region = \"us\"):\n   data = { 'grant_type': 'client_credentials' }\n   response = requests.post('https://%s.battle.net/oauth/token' % region,\n                            data = data,\n                            auth = (client_id, client_secret))\n   return response.json()\n\n# Get Malf and connected realms data\ndef get_malfurion(search):\n   search = search\n   response = requests.get(search)\n   return response.json()[\"auctions\"]\n \nresponse = create_access_token(r['client_id'], r['client_secret'])\ntoken = response['access_token']\n```\n:::\n\n\nNow, I specify a few more parameters including my realm ID. I'm on a connected realm for Malfurion (+ a bunch of others I never remember). Each of our realms returns it's own ID, but for some reason this id (1175) was the only one that actually returned data. So if you're on a connected realm, you might have to experiment using other realm's ids.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set up search query with Malfurion realm ID\nid <- 1175\ntoken <- py$token\nsearch <- glue(\"https://us.api.blizzard.com/data/wow/connected-realm/{id}/auctions?namespace=dynamic-us&locale=en_US&access_token={token}\")\n```\n:::\n\n\nNow we can actually access the data! I add a few features to the data to capture when I pulled the data. I also write the file to csv. I wrote the file to csv because, even though I should be able to access a Python data frame in R through the reticulate package, I was having trouble getting it to work. My work around was to write the file, reload it so R had access to it, write the R data frame to a SQL database, then delete the csv. You can also see that I do a little bit of cleaning with the data. Depending on the type of item posted, it either has a `unit_price` or a `buyout`, but I want just a single column for cost. I then convert the cost from how it is listed in units of bronze to units of gold (100 bronze in a silver, 100 silver in a gold). \n\n\n::: {.cell}\n\n```{.python .cell-code}\nauction_data = get_malfurion(r['search'])\nauction_data = pd.DataFrame(auction_data)\n# Expand the item column\nauction_data = auction_data.rename(columns={\"id\": \"auction_id\",})\nauction_data = pd.concat([auction_data.drop(['item'], axis=1), auction_data['item'].apply(pd.Series)], axis=1)\n\n# Drop 'bonus_list' and 'modifiers' \n#   These are subgroups of an equipable item with the bonus stats (intellect agility, strength, etc)\nauction_data['collection_year'] = datetime.now().strftime('%Y')\nauction_data['collection_month'] = datetime.now().strftime('%m')\nauction_data['collection_day'] = datetime.now().strftime('%d')\nauction_data['collection_hour'] = datetime.now().strftime('%H')\nfilename = datetime.now().strftime('F:/Documents/WoWAH/data/Malfurion_NA-%Y-%m-%d-%H-%M.csv')\ntablename = datetime.now().strftime('Malfurion_NA-%Y-%m-%d-%H-%M')\nauction_data.to_csv(filename, index = False)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the data into R to clean it a bit more (since I'm less good at cleaning)\n# in python\nauction_df <- read_csv(py$filename)\n\n# Unit prices are for stackable items \nauction_df <- auction_df %>% \n   mutate(unit_price = replace_na(unit_price, 0),\n          buyout = replace_na(buyout, 0),\n          cost = unit_price + buyout,\n          cost_g = cost / 10000)\n\nwrite_csv(auction_df, py$filename)\n\n# Add to database\nlibrary(RSQLite)\nlibrary(dbplyr)\n\ncon <- dbConnect(RSQLite::SQLite(), \"data/WoWAH_database.sqlite\")\ndbWriteTable(con, py$tablename, auction_df)\nfile.remove(py$filename) # delete the csv file once it is in the database.\n```\n:::\n\n\nThe next step was to query the API for information about each item on the auction house. The auction house data doesn't contain the human-readable name for the item, it just has an item ID number. So I have to use a seperate query to get the item information. To do this efficiently and prevent constant excessive querying of the API, I built a new data base table to hold this information and then build on this table if I ever find an item in my most recent auction house scan that isn't in the item info database. \n\nTo do this, I first start by finding all the item ids that aren't already in the data base. Then, I split this list of item ids into chunks with a max of 50 item ids per chunk. I found this number through some trial and error querying the API - any queries requesting information on more than 50 items at a time only returned a max of 50 items. Then I construct as many queries as there are chunks. The first time I ran this, it took a while because I was building the item information database from scratch. But it's pretty uncommon for previously unseen items to show up in the auction house, so this doesn't actually run that often anymore.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for any items in the newly scraped AH data that isn't in\n# the current database\nitem_db <- tbl(con, \"item_name\")\nauction_df <- tbl(con, py$tablename)\nnot_in_db_df <- anti_join(auction_df, item_db, by = \"id\") %>% \n   collect()\nid <- 1175\ntoken <- py$token\n# Format item info to request from API\nitem_ids <- unique(not_in_db_df$id)\n\n# Helper function to chunk id vector\ncreateChunks <- function(x, elements.per.chunk){\n   # plain R version\n   split(x, rep(seq_along(x), each = elements.per.chunk)[seq_along(x)])\n\n}\n\nitem_chunks <- createChunks(item_ids, 50)\nsearch_chunks <- map(item_chunks, function(.x) {\n   id_char <- paste(.x, collapse = \"||\")\n   glue(\"https://us.api.blizzard.com/data/wow/search/item?namespace=static-us&locale=en_US&orderby=id&&_pageSize=1000&id={id_char}&_&access_token={token}\")\n})\n# Need this to be a Python list so I remove names (a named list becomes a python dict)\nsearch_chunks <- unname(search_chunks)\n```\n:::\n\n\nNow that I have the item-query-chunks, I use some more Python code to actually do the query.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas\nitem_names = []\n\nfor search_url in r['search_chunks']:\n   response = requests.get(search_url).json()\n   response_df = pandas.io.json.json_normalize(response, record_path = ['results'])\n   item_names.append(response_df)\n\n\nitem_df = pd.concat(item_names, ignore_index = True, axis = 0, sort = True)\n```\n:::\n\n\nAnd now I clean up the item table a bit and then write it/append it to the database.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Item classes index an\nnew_item_df <- py$item_df %>% \n   select(data.id, data.is_equippable, data.is_stackable, data.level, data.max_count,\n          data.media.id,  data.name.en_US,\n          data.purchase_price, data.required_level, data.sell_price) %>% \n   rename_all(~stringr::str_replace(., \"^data.\", \"\")) %>% \n   rename_at(.vars = vars(ends_with(\".en_US\")),\n             ~ sub('[.]en_US$', '', .))\n\ndbAppendTable(con, name = \"item_name\", value = new_item_df)\n```\n:::\n\n\nFor a while I was running this on a computer with limited storage, so I set it up to delete any data older than 4 weeks. I've since turned off this code chunk, as I moved the script over to a computer with much more storage so I'm not really concerned about the amount of data I'm keeping. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# I don't need to keep all the data, let's keep a month's worth\ntable_list <- data.frame(table_list = dbListTables(con))\ntable_list <- filter(table_list, table_list %!in% c(\"item_name\", \"item_db\"))\ntable_list$date_times <-\n   str_extract(table_list$table_list,\n               \"[0-9]{4}-[0-9]{2}-[0-9]{2}-[0-9]{2}-[0-9]{2}\")\ntable_list$date_times <- ymd_hm(table_list$date_times)\n# Just in case the tables aren't in order for date time\ntable_list$index <- 1:nrow(table_list)\ntable_list <- arrange(table_list, desc(date_times))\nkeep_tables <- slice(table_list, 1:672)\nremove_tables <- table_list[table_list$index %!in% keep_tables$index, ]\nremove_tables <- remove_tables$table_list\n\nmap(remove_tables, ~dbRemoveTable(con, .x))\ndbDisconnect(con)\n```\n:::\n\n\nLastly, I set up some email alerts! Here I just have an example for two items I was watching: elethium ore and widowbloom. I had to be careful with this, as I initially set the alerts to trigger at prices that occurred to frequently and Google flagged my account for inappropriate use! There's probably some other service I could use and not worry about it, but I had the threshold set too high anyway and was getting more emails than I could act on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwatched_items <- c('Elethium Ore', 'Widowbloom')\nwatched_item_df <- auction_df %>% \n   left_join(item_db, by = 'id', copy = TRUE) %>% \n   filter(name %in% watched_items) %>% \n   select(name, cost_g, quantity) %>% \n   group_by(name, cost_g) %>% \n   summarise(quantity = sum(quantity, na.rm = TRUE)) %>% \n   collect()\n\nemail_items <- watched_item_df %>% \n   group_by(name) %>% \n   summarise(min_cost = min(cost_g)) %>%\n   left_join(., watched_item_df, by = c('name', 'min_cost' = 'cost_g')) %>% \n   mutate(buy = case_when(name == 'Elethium Ore' & min_cost < 60 ~ 1,\n                          name == 'Widowbloom' & min_cost < 16 ~ 1,\n                          TRUE ~ 0)) %>% \n   filter(buy == 1)\nitems_to_buy <- paste0(email_items$name, \": \", email_items$quantity, collapse = \", \")\nnames(items_to_buy) <- 'items_to_buy'\n# Set up text to send the giver their assigned gift recipient\nif (nrow(email_items) > 0) {\nbody <- \"Dear Jake,\n\n<p>Consider purchasing the following items:\n<br> {items_to_buy}\n\n\"\n\n# Set up email\nemail <- envelope()\nemail <- email %>% \n   from(\"your.account.for.sending.alerts@email.com\") %>% \n   to('your.account.for.receiving@emailcom')\n\nemail <- email %>% \n   subject(paste0('WoW AH ', Sys.time()))\n\nemail <- email %>% \n   html(glue_data(items_to_buy, body))\n\n\nsmtp <- server(host = \"smtp.gmail.com\",\n               port = 587,\n               username = \"your.account.for.sending.alerts@email.com\",\n               password = \"your.password.for.email.authentication\")\nsmtp(email, verbose = TRUE)\n}\n```\n:::\n\n\nAnd that's it! Hopefully there's enough information here for someone else to get a similar system up and running. Next steps I'd like to do are to build a Shiny dashboard for visualizing some of this data. I'm also interested in learning more about time series analysis, so I'm planning to use this data in some self-teaching exercises. Reach out to me on twitter if you have questions! \n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}