{
  "hash": "73d7c1abf7cc39f181012634637bf83f",
  "result": {
    "markdown": "---\ntitle: \"Messing around with XGBoost\" # <---- UPDATE ME\ndescription:\n  I've been reading about XGBoost and wanted to take it for a ride # <---- UPDATE ME\nauthor:\n  - name: \"Jake Wittman\"\n    url: https://wittja01.github.io/website/\ndate: 2022-08-26\nimage: preview.jpg  # <---- UPDATE ME \nlicense: CC BY\nrepository_url: https://github.com/wittja01/website\ncategories: machine learning, workthrough\nparams:\n  slug: messing-around-with-xgboost\n  date: 2022-08-26\n  repo: wittja01/website\n  site: https://wittja01.github.io/website\ndraft: false\n---\n\n```{=html}\n<!----\nUse \nrefinery::use_article_template(\n    template = \"_templates/standard_template.Rmd\",\n    slug = \"how-cold-is-too-cold\", \n    date = \"2022-01-11\",\n    renv_new = TRUE\n)\nto generate new post\n\nchecklist:\n  - check the \"update me\" messages in YAML above\n  - initialise the _renv folder with refinery::renv_new(\"name of post folder\")\n  - populate the lockfile with refinery::renv_snapshot(\"name of post folder\")\n  - update the _renv folder from snapshot with refinery::restore(\"name of post folder\")\n\n---->\n```\n\n\n<!--------------- post ----------------->\n\nFor learning how to use XGBoost in R, I've settled on the wine quality data set.[^1] This data contains a number of physiochemical variables pertaining to the different types of wine as well as a quality score (between 0 and 10). I know very little about wine, and mostly just drink my wine from a box, but I thought it sounded like a fun dataset to use. Let's explore the data a bit before we dive into working with XGBoost. I'll start by combining the two datasets into a single wine dataset.\n\n## Opening the bottle\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'magrittr' was built under R version 4.0.5\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nred_wine <- read.csv('data/winequality-red.csv', sep = \";\")\nwhite_wine <- read.csv('data/winequality-white.csv', sep = \";\")\nwine <- bind_rows(red = red_wine, white = white_wine, .id = 'wine_type') %>% \n   mutate(wine_type = as.factor(wine_type)) %>% \n   as_tibble()\n```\n:::\n\n\nLet's get some quick pairs plots up. Trying to plot all 13 variables on one plot is just too messy, and I don't care too much about the pairs plots for our different phsyiochemical variables right now, so I split this plot into two so they're a bit easier to see. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'GGally' was built under R version 4.0.5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n```{.r .cell-code}\nggpairs(wine, c(13, 1:7), mapping=ggplot2::aes(colour = wine_type))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggpairs(wine, c(13, 1, 8:12), mapping=ggplot2::aes(colour = wine_type))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\nLooking at these plots, it looks like we have some significant correlation between our physiochemical variables and either quality or wine type. I think we could probably expect physiochemical variables to vary between red and white wines. Sulphates, for example, tend to be higher in red wine than in white wines. Sulfur dioxide appears to be slightly highe in white than in red. As far as quality goes, more sulphates are correlated with higher quality red wines, but when considered overall in the wine have almost a very small (albeit significant) correlation with quality. This suggests that different variables will have different predictive power depending on if the wine is red vs white, so if we were to train models separately on each we would expect the models would not look the same. However, if predicting quality is the goal I think I want to include wine type. We will build models for predicting both quality and wine type.\n\n## What is XGBoost?\n\nXGBoost is a gradient boosting algorithm (it stands for eXtreme Gradient Boosting) used in supervised learning, where we are interested in using multiple features $x_i$ to predict our target $y_i$. Like most machine learning methods, it requires picking a loss function to express how predictive our model is given the data and a way to regularize our model, or to ensure that we aren't overparameterizing the model and fitting noise. In the simplest terms I can manage, what we are doing is fitting a really simple classification or regression tree, calculating the errors from that tree, then we build a model to predict those errors and add the most recent model to our base model to form an ensemble of models. Repeat ad nasuem (well, until we've optimized our loss/regularization function). \n\nI'll be using the `tidymodels` package in R for fitting and tuning this model. It's another package I've been meaning to learn, and here's a great opportunity. For this, I'm following along with Julia Silge's [post](https://juliasilge.com/blog/xgboost-tune-volleyball/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages -------------------------------------- tidymodels 1.0.0 --\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nv broom        1.0.0     v rsample      1.1.0\nv dials        1.0.0     v tibble       3.1.8\nv infer        1.0.3     v tidyr        1.2.0\nv modeldata    1.0.0     v tune         1.0.0\nv parsnip      1.0.1     v workflows    1.0.0\nv purrr        0.3.4     v workflowsets 1.0.0\nv recipes      1.0.1     v yardstick    1.0.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'purrr' was built under R version 4.0.5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.0.5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard()   masks scales::discard()\nx tidyr::extract()   masks magrittr::extract()\nx dplyr::filter()    masks stats::filter()\nx dplyr::lag()       masks stats::lag()\nx purrr::set_names() masks magrittr::set_names()\nx recipes::step()    masks stats::step()\n* Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n\n```{.r .cell-code}\nset.seed(42)\nwine_split <- initial_split(wine, strata = wine_type)\nwine_train <- training(wine_split)\nwine_test <- testing(wine_split)\n```\n:::\n\n\nNow we can set up the model specification, including the hyperparameters we're going to tune. I'll just note that I'm doing a regression tree here, since quality is between 0 and 10. Later, we'll try predicting wine type and do a classification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <- boost_tree(\n   trees = 1000,\n   tree_depth = tune(),\n   min_n = tune(),\n   loss_reduction = tune(),\n   sample_size = tune(),\n   mtry = tune(),\n   learn_rate = tune()\n) %>% \n   set_engine('xgboost') %>% \n   set_mode(\"regression\")\n```\n:::\n\n\nA bit on each hyperparameter. The `trees` hyperparameter is pretty straightforward: it's the number of trees in our ensemble. `min_n` is the minimum number of data points in a node in order for that node to be split. So, say `min_n` is set to 5. If a node ends up with 4 observations in it, it will not be split further. `loss_reduction` will determine how much of a loss we should be looking for when we split a node. This helps prevent overfitting and splitting nodes where the change in loss is negligible. `mtry` is the number of predictors to be randomly sampled when doing each split. This also helps prevent overtuning in decision trees. The `learn_rate` parameter determines how much the algorithm adapts from tree to tree. I'm not 100% sure what `sample_size` means - the help file for `boost_tree()` says its the number for the number (or proportion) of data that is exposed to the fitting routine. I'm guessing that means that only a certain amount of training data is given to the model each time?\n\nAnyway, this is a lot of hyperparameters and we need to specify some numbers to try. Julia uses a latin hypercube, so we'll go ahead and do the same.\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_grid <- grid_latin_hypercube(\n   tree_depth(),\n   min_n(),\n   loss_reduction(),\n   sample_size = sample_prop(),\n   finalize(mtry(), wine_train),\n   learn_rate(),\n   size = 30\n)\n\nxgb_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 x 6\n   tree_depth min_n loss_reduction sample_size  mtry learn_rate\n        <int> <int>          <dbl>       <dbl> <int>      <dbl>\n 1          5    36       1.12e- 5       0.525     9   3.76e- 7\n 2          8    34       1.69e+ 0       0.952     4   5.20e- 2\n 3          4    24       2.76e- 1       0.240    10   9.91e-10\n 4         14    37       1.32e- 2       0.767     6   1.16e- 2\n 5         11    32       2.05e- 3       0.926     5   9.37e- 4\n 6         13    12       2.06e+ 1       0.725     5   7.40e- 8\n 7          7    10       1.53e- 9       0.645    11   3.24e- 3\n 8          3    16       3.59e- 7       0.257     4   2.10e- 9\n 9         12     9       2.17e-10       0.551    12   1.19e-10\n10         14    19       4.14e- 9       0.192     8   1.94e- 8\n# ... with 20 more rows\n```\n:::\n:::\n\n\nI've not seen finalize before, but I guess it's used to specify how many possible parameters can be sampled from for the `mtry` parameter. \n\nNow we get to set up the workflow and the cross-validation samples for tuning. Because I want to use wine type as a predictor, I think I need to add an extra recipe step.\n\n::: {.cell}\n\n```{.r .cell-code}\nwine_rec <- recipe(quality ~ ., data = wine_train) %>% \n   step_dummy(wine_type) %>% \n   step_normalize(all_numeric_predictors())\n\nxgb_wf <- workflow(wine_rec) %>%\n   add_model(xgb_spec)\nxgb_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: boost_tree()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_dummy()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n:::\n\n```{.r .cell-code}\nwine_folds <- vfold_cv(wine_train, strata = wine_type)\nwine_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  10-fold cross-validation using stratification \n# A tibble: 10 x 2\n   splits             id    \n   <list>             <chr> \n 1 <split [4384/488]> Fold01\n 2 <split [4384/488]> Fold02\n 3 <split [4384/488]> Fold03\n 4 <split [4385/487]> Fold04\n 5 <split [4385/487]> Fold05\n 6 <split [4385/487]> Fold06\n 7 <split [4385/487]> Fold07\n 8 <split [4385/487]> Fold08\n 9 <split [4385/487]> Fold09\n10 <split [4386/486]> Fold10\n```\n:::\n:::\n\n\nNow we get to run the model! Let's set up the parallel backend to speed things up. I'm doing this with a 12-core (24-hypercores) processor, so hopefully it'll be speedy. Setting it to use 10 cores so I can still use my computer in the meantime.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel(cores = 10)\n\nxgb_res <- tune_grid(\n   xgb_wf,\n   resamples = wine_folds,\n   grid = xgb_grid,\n   control = control_grid(save_pred = TRUE) # For exploring predictions\n)\nxgb_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 x 5\n   splits             id     .metrics           .notes           .predictions\n   <list>             <chr>  <list>             <list>           <list>      \n 1 <split [4384/488]> Fold01 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 2 <split [4384/488]> Fold02 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 3 <split [4384/488]> Fold03 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 4 <split [4385/487]> Fold04 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 5 <split [4385/487]> Fold05 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 6 <split [4385/487]> Fold06 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 7 <split [4385/487]> Fold07 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 8 <split [4385/487]> Fold08 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n 9 <split [4385/487]> Fold09 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n10 <split [4386/486]> Fold10 <tibble [60 x 10]> <tibble [1 x 3]> <tibble>    \n\nThere were issues with some computations:\n\n  - Warning(s) x10: A correlation computation is required, but `estimate` is constant...\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n:::\n:::\n\n\nLet's visualize our different hyperparameters and the associated metric. We'll use RMSE in this case.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res %>% \n   collect_metrics() %>% \n   filter(.metric == 'rmse') %>% \n   select(mean, mtry:sample_size) %>% \n   pivot_longer(mtry:sample_size,\n                values_to = 'value',\n                names_to = 'hyperparameter') %>% \n   ggplot(aes(value, mean, color = hyperparameter)) +\n   geom_point(alpha = 0.8, show.legend = FALSE) +\n   facet_wrap(~hyperparameter, scales = 'free_x') +\n   labs(x = NULL, y = 'RMSE')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLearn rate seems to be inversely proportional to loss reduction in terms of the effect on the RMSE. The other parameters are a bit all over the place, suggesting multiple possible best parameter values for those. Let's see what was actually the best\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_res, 'rmse')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 12\n   mtry min_n tree_depth learn_rate loss_r~1 sampl~2 .metric .esti~3  mean     n\n  <int> <int>      <int>      <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1    10     4         12    0.0134   2.34e-6   0.499 rmse    standa~ 0.607    10\n2     6    37         14    0.0116   1.32e-2   0.767 rmse    standa~ 0.630    10\n3    12    16          5    0.0415   1.88e-4   0.830 rmse    standa~ 0.640    10\n4     4    34          8    0.0520   1.69e+0   0.952 rmse    standa~ 0.645    10\n5    11    10          7    0.00324  1.53e-9   0.645 rmse    standa~ 0.698    10\n# ... with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names 1: loss_reduction, 2: sample_size, 3: .estimator\n```\n:::\n:::\n\n\nI don't show it here, but the top two models were the same whether we chose RMSE or RSQ as our metric.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rmse <- select_best(xgb_res, 'rmse')\nbest_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 7\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .config          \n  <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>            \n1    10     4         12     0.0134     0.00000234       0.499 Preprocessor1_Mo~\n```\n:::\n:::\n\n\nNow we get our final model and see which variables appear to be the most important, then we'll go ahead and test the model on our test data.\nFor some reason, wine_type isn't showing up in the VI plot below. At first I thought it was because I needed to convert it to a dummy encoding (hence the recipe with `step_dummy()` previously). But that still doesn't solve the issue. If anyone knows why, tweet at me! (I haven't figured out how to turn on comments yet, or if I want to). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_xgb <- finalize_workflow(\n   xgb_wf,\n   best_rmse\n)\nfinal_xgb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: boost_tree()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_dummy()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  trees = 1000\n  min_n = 4\n  tree_depth = 12\n  learn_rate = 0.0134175731334054\n  loss_reduction = 2.34317269580068e-06\n  sample_size = 0.499168963264674\n\nComputational engine: xgboost \n```\n:::\n\n```{.r .cell-code}\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'vip' was built under R version 4.0.5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vip'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:utils':\n\n    vi\n```\n:::\n\n```{.r .cell-code}\nfinal_xgb %>% \n   fit(data = wine_train) %>% \n   extract_fit_parsnip() %>% \n   vip(geom = 'point')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nInteresting that alcohol seems far and away the most important predictor of quality. Looking back at our EDA plots, it doesn't jump out at me as particularly predictive, but then again nothing really did. Maybe the raters just enjoyed getting drunk faster. I'm not sure what volatile acidity means (remember, I don't know anything about wine although I'm definitely going to drink some after I finish this).\n\nNow we can test the model!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_result <- last_fit(final_xgb, wine_split)\ncollect_metrics(final_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.601 Preprocessor1_Model1\n2 rsq     standard       0.512 Preprocessor1_Model1\n```\n:::\n:::\n\n\nAn RMSE of 0.625 seems pretty good, but lets make a plot of our predictions. On this plot, I've overlaid the grey dotted line which is what the model should be predicting if it were correct all the time. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_result %>% \n   collect_predictions() %>% \n   ggplot(aes(x = quality, y = .pred)) +\n   geom_point() +\n   geom_line(aes(quality, quality), \n             linetype = 'dashed',\n             color = 'grey',\n             inherit.aes = FALSE) +\n   theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nSeems pretty clear that our model thinks bad wines are better than they actually are and great wines are worse than they actually were rated. Our model needs to drink more bad/great wines! We can tell from the plot that there are definitely fewer wines present at the ends of the spectrum, but lets take a look to see.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine %>% \n   group_by(quality) %>% \n   count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 x 2\n# Groups:   quality [7]\n  quality     n\n    <int> <int>\n1       3    30\n2       4   216\n3       5  2138\n4       6  2836\n5       7  1079\n6       8   193\n7       9     5\n```\n:::\n\n```{.r .cell-code}\nhist(wine$quality)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nMakes sense that the model may not perform as well out in the regions where it doesn't have as much information. I know I said I was going to predict wine type next, but I'm tired. I'll save that for another post!\n\n[^1]:  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}