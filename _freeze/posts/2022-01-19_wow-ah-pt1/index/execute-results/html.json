{
  "hash": "90de27ace196c029aa9fa02d799fffb8",
  "result": {
    "markdown": "---\ntitle: \"Download data from the World of Warcraft auction house REDUX\" # <---- UPDATE ME\ndescription:\n  To learn a bit more Python and make my scraping more reliable, I rewrote my scraping and database building scripts in Python. # <---- UPDATE ME\nauthor:\n  - first_name: \"Jake\"\n    last_name: \"Wittman\"\n    url: https://wittja01.github.io/website/\ndate: 2022-01-19\nimage: app_preview.jpg  # <---- UPDATE ME \ncreative_commons: CC BY\ncitation_url: https://wittja01.github.io/website/wow-ah-pt1 \nrepository_url: https://github.com/wittja01/website\noutput:\n  distill::distill_article:\n    self_contained: false\nparams:\n  slug: wow-ah-pt1\n  date: 2022-01-19\n  repo: wittja01/personal_website\n  site: https://wittja01.github.io/personal_website\n---\n\n\n<!----\nUse \nrefinery::use_article_template(\n    template = \"_templates/standard_template.Rmd\",\n    slug = \"how-cold-is-too-cold\", \n    date = \"2022-01-11\",\n    renv_new = TRUE\n)\nto generate new post\n\nchecklist:\n  - check the \"update me\" messages in YAML above\n  - initialise the _renv folder with refinery::renv_new(\"name of post folder\")\n  - populate the lockfile with refinery::renv_snapshot(\"name of post folder\")\n  - update the _renv folder from snapshot with refinery::restore(\"name of post folder\")\n\n---->\n\n\n<!--------------- setup post ----------------->\n\n\n\n\n\n\n<!--------------- post ----------------->\n\nIn [this post](https://wittja01.github.io/website/posts/2021-12-03_time-is-money-friend/) I described how I wrote some Python and R code to scrape the WoW auction house API to get auction data. The code felt kind of fragile though. Changes in one part of the script would cause more cascades through the code than I was comfortable with (maybe it's because of my programming skills moreso than trying to combine two languages), so I wanted to rewrite into one language to try and avoid that. I decided on Python because I wanted to get more comfortable using it, and using Pandas, so I rewrote the whole process of downloading the data, cleaning it, and building the database in Python. I'll walk through that code here, and then in a future post I'll talk about the R Shiny app I built using this data.\n\nThe first of my scripts pulls the auction house data from the Blizzard API hourly. I use a config file to store my API keys for both the dropbox API and Blizzard API. I wrote a couple functions to pull the data from the Blizzard API using information specific to the server I play on. You could repurpose this code for your own server, but you'd need to update the `id` variable for your server.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#########\n# This script uses access tokens for dropbox and Blizzard API keys to scrape\n# data from the WoW auction house and upload the csv to drop box.\n# The pull_data.py script will pull the data from dropbox, delete it once pulled\n# and add it to the database. \n#########\nimport requests\nimport os\nfrom datetime import datetime\nimport pandas as pd\nimport dropbox\nfrom decouple import config\n\nDROPBOX_ACCESS = config('DROPBOX_ACCESS')\ndbx = dropbox.Dropbox(DROPBOX_ACCESS)\n\n# Create access toeken\ndef create_access_token(client_id, client_secret, region = \"us\"):\n   data = { 'grant_type': 'client_credentials' }\n   response = requests.post('https://%s.battle.net/oauth/token' % region,\n                            data = data,\n                            auth = (client_id, client_secret))\n   return response.json()\n\n# Get Malf and connected realms data\ndef get_malfurion(search):\n   search = search\n   response = requests.get(search)\n   return response.json()[\"auctions\"]\n\nCLIENT_ID_WOW = config('CLIENT_ID_WOW')\nCLIENT_SECRET_WOW = config('CLIENT_SECRET_WOW')\nresponse = create_access_token(CLIENT_ID_WOW, CLIENT_SECRET_WOW)\ntoken = response['access_token']\n\n# ID for NA-Malfurion server\nid = 1175\nsearch = f\"https://us.api.blizzard.com/data/wow/connected-realm/{id}/auctions?namespace=dynamic-us&locale=en_US&access_token={token}\"\n\n# Get Data\nauction_data = get_malfurion(search)\n```\n:::\n\n\nNext steps are to clean the data. We take the JSON file and turn it into a Pandas dataframe. I rename some of the columns, expand others, and fill in NA values. There are two variables that represent how much an auction is posted for: `buyout` and `unit_price`. In game, some items are \"stackable\", like potions. You can stack 20 of them in one inventory spot. Other items are not stackable, like armor. A pair of boots occupies a single inventory space and cannot stack. The buyout price is what is used for unstackable items, like boots, and the unit_price is the cost of a stackable item. The distinction isn't really important for my purposes, so I combine them into a single `cost` column and then convert that to cost in gold. I also add some datetime columns for future use in modeling. I could do this more efficiently and just have a single datetime column from which I could derive the others, but right now storage space isn't a concern. At the end of the cleaning, I write the dataframe to a csv file.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Convert data to a dataframe\nauction_data = pd.DataFrame(auction_data)\n\nauction_data = auction_data.rename(columns={\"id\": \"auction_id\",})\n# Expand the item column\nauction_data = pd.concat([auction_data.drop(['item'], axis=1), auction_data['item'].apply(pd.Series)], axis=1)\nauction_data['id'] = auction_data['id'].map(int).map(str) # Convert to int to remove trailing 0, then to string\n# Unit prices are for stackable items, buyout is for unstackable. I just want a single gold cost column\nauction_data['buyout'] = auction_data['buyout'].fillna(0)\nauction_data['unit_price'] = auction_data['unit_price'].fillna(0)\nauction_data['cost'] = auction_data['buyout'] + auction_data['unit_price']\n# Cost is in copper, convert to gold\nauction_data['cost_g'] = auction_data['cost'] / 10000\n# Remove rows for pets. I don't care about pets\n# And remove columns about pets\nis_pet_mask = auction_data['pet_breed_id'].isna()\nauction_data = auction_data[is_pet_mask]\nauction_data.drop(columns = ['pet_breed_id', 'pet_level', 'pet_quality_id', 'pet_species_id'], inplace = True)\n\n#   These are subgroups of an equipable item with the bonus stats (intellect agility, strength, etc)\n# Make one date time column and also 4 columns for each relevant piece\nauction_data['date_time'] = datetime.now()\nauction_data['collection_year'] = datetime.now().strftime('%Y')\nauction_data['collection_month'] = datetime.now().strftime('%m')\nauction_data['collection_day'] = datetime.now().strftime('%d')\nauction_data['collection_hour'] = datetime.now().strftime('%H')\nfilename = datetime.now().strftime('Malfurion_NA-%Y-%m-%d-%H-%M.csv')\n```\n:::\n\n\nHere, I upload the csv file to Dropbox and delete the csv file locally. I do this because eventually I want to migrate this script to something like a Raspberry Pi so I can have it constantly run. Right now it's running constantly on my personal PC, and I don't want to do that forever. I want to be able to turn on my PC and have it scrape what's been collected in Dropbox since the last time. I tried automating this with Github Actions, but found that Github Actions would not run every hour (at least, not for a free plan). You can *ask* it to run hourly, but depending on demand it may or may not. I was getting some scrapes occurring within 20 minutes of each other and others hours apart, which isn't useful for an API that is updated hourly.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndropbox_filename = \"/\" + filename\nauction_data.to_csv(filename, index = False)\n\nwith open(filename, 'rb') as f:\n   dbx.files_upload(f.read(), dropbox_filename)\n\nos.remove(filename)\n```\n:::\n\n\nI use Windows Task Scheduler to run the above code ever hour on the hour. Then I have a script that pulls the data from Dropbox and places it into a SQLite database, which I'll detail below. At this point, it's overkill (why repull the data I already had?) but like I said earlier, I eventually want to migrate the first script to a different computer.\n\nAgain, I need the config file to get the Dropbox API keys. Figuring out how to download from Dropbox was a bit of a pain in the ass. The documentation isn't great for a beginner, but after some extensive googling I think I figured it out. \n\n\n::: {.cell}\n\n```{.python .cell-code}\n############\n# This script pulls WoW AH data from my dropbox to my machine, where\n# I'm building a database of this data. \n############\nimport os\nfrom datetime import datetime\nimport pandas as pd\nimport dropbox\nfrom decouple import config\nimport pandas\nimport sqlite3\nDROPBOX_ACCESS = config('DROPBOX_ACCESS')\ndbx = dropbox.Dropbox(DROPBOX_ACCESS)\n# Had some issues with this script. The only workaround I could find\n# was to specify the full directory path.\nmain_dir = 'F:/Documents/WoWAH_py/WoWAH_python/'\n# Download csvs from dropbox\n# These go in the temp_csvs folder until they're added to the databse\n# then they're deleted\ndb_files = dbx.files_list_folder(\"\")\nfor i in db_files.entries:\n    with open(main_dir + 'data/temp_csvs/'+ i.name, \"wb\") as f:\n        metadata, res = dbx.files_download(i.path_lower)\n        f.write(res.content)\n```\n:::\n\n\nNow, I connect to the local database and append the hourly data to the existing auctions table. This should also work if the auctions table doesn't exist already; it'll generate it the first time this is run. Then I delete the csv files from Dropbox, since I have a free plan and don't have unlimited space.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Connect to database and add csvs\nconn = sqlite3.connect(main_dir + 'data/WoWAH_db.sqlite')\n\nah_csvs = os.listdir(main_dir + 'data/temp_csvs')\ncurs = conn.cursor() # Create cursor w/e that is\n\nfor i in ah_csvs:\n    temp_file = pandas.read_csv(main_dir + 'data/temp_csvs/' + i)\n    temp_file['id'] = temp_file['id'].map(str)\n    temp_file['date_time'] = pd.to_datetime(temp_file['date_time'])\n    temp_file.to_sql('auctions', conn, if_exists='append', index = False)\n\n# Once files are in the database, go ahead and delete from dropbox\nfor i in db_files.entries:\n    dbx.files_delete(\"/\" + i.name)\n\n# And delete  csvs\nfor i in ah_csvs:\n    os.remove(main_dir + 'data/temp_csvs/' + i)\n```\n:::\n\n\nThis last step queries the database to get the most recent 4 weeks of data and writes it to a csv, which is used locally by a Shiny app I've built. One thing to note is that when I pull the auction data, I don't get the name of the items up for auction. Instead the items all have an id number. I have to use that number in an additional API call to get the item name and other information about the items. So in a later post, I'll detail how I use unique item ids from the auctions table to build an item table in the database, update that table as new, not-seen-before items show up in the auctions table. You can see that I use a left join to bring over relevant columns from the item_id table.\n\nYou can find a demo of this app [here](https://jake-wittman.shinyapps.io/wowah_app/). The demo uses a subset of old data, since I'm limited to a free plan with only so much storage. Eventually I'll update it to deploy a subset of new data ever hour, but that's for a later time!\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Write a csv to file that contains the most recent 2 (4?) weeks of data\nquery = curs.execute(\"\"\"\n    SELECT auctions.auction_id,\n        auctions.quantity,\n        auctions.time_left,\n        auctions.date_time,\n        auctions.cost_g,\n        item_id.name,\n        item_id.is_stackable,\n        item_id.is_equippable\n    FROM auctions\n    LEFT JOIN item_id ON auctions.id = item_id.id\n    WHERE date_time BETWEEN datetime('now', '-1 month') and datetime('now', 'localtime')\n    \"\"\")\ncols = [column[0] for column in query.description]\nresults = pd.DataFrame.from_records(data = query.fetchall(), columns = cols)\nresults.to_csv(main_dir + '/data/latest_month_auctions.csv', index = False)\n# Write a smaller version of the data to upload with the app as a test\nquery = curs.execute(\"\"\"\n    SELECT auctions.auction_id,\n        auctions.quantity,\n        auctions.time_left,\n        auctions.date_time,\n        auctions.cost_g,\n        item_id.name,\n        item_id.is_stackable,\n        item_id.is_equippable\n    FROM auctions\n    LEFT JOIN item_id ON auctions.id = item_id.id\n    WHERE date_time BETWEEN datetime('now', '-1 month') and datetime('now', 'localtime')\n    AND name IN ('Rising Glory',\n            'Widowbloom',\n            'Marrowroot',\n            \"Vigil's Torch\",\n            'Death Blossom',\n            'Nightshade',\n            'Laestrite Ore',\n            'Elethium Ore',\n            'Solenium Ore',\n            'Oxxein Ore',\n            'Phaedrum Ore',\n            'Sinvyr Ore',\n            'Angerseye',\n            'Oriblase',\n            'Umbryl',\n            'Desolate Leather',\n            'Callous Hide',\n            'Pallid Bone',\n            'Gaunt Sinew',\n            'Heavy Desolate Leather',\n            'Heavy Callous Hide',\n            'Shrouded Cloth',\n            'Lightless Silk',\n            'Soul Dust',\n            'Sacred Shard',\n            'Eternal Crystal')\n    \"\"\")\ncols = [column[0] for column in query.description]\nresults = pd.DataFrame.from_records(data = query.fetchall(), columns = cols)\nresults.to_csv(main_dir + 'wowah_app/app_data/latest_month_auctions.csv', index = False)\n\nconn.close()\n```\n:::\n\n\nSo this details my automated workflow to pull WoW AH data with Python. It's set up so I can eventually migrate it to a different computer, so it's got some redundancies in how it functions right now. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}