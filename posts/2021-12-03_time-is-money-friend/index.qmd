---
title: "Downloading data from the World of Warcraft Auction House" # <---- UPDATE ME
description:
  How I leveraged my programming knowledge to avoid paying real money for a video game # <---- UPDATE ME
author:
  - first_name: "Jake"
    last_name: "Wittman"
    url: https://jake-wittman.github.io/website/
date: 2021-12-03
image: org_ah.jpg  # <---- UPDATE ME 
citation_url: https://jake-wittman.github.io/website/time-is-money-friend 
repository_url: https://github.com/jake-wittman/WoWAH
params:
  slug: time-is-money-friend
  date: 2021-12-03
  repo: jake-wittman/WoWAH
  site: https://jake-wittman.github.io/website
---

<!----

checklist:
  - check the "update me" messages in YAML above
  - initialise the _renv folder with refinery::renv_new("name of post folder")
  - populate the lockfile with refinery::renv_snapshot("name of post folder")
  - update the _renv folder from snapshot with refinery::restore("name of post folder")

---->


<!--------------- setup post ----------------->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```


<!--------------- post ----------------->

In my free time, I enjoy playing a little World of Warcraft. Some of my earliest childhood memories are sitting next to my dad while he would play RTS games like Warcraft 2 and Command & Conquer, and when I got a little bit older I started playing myself. Warcraft was always a favorite - I spent weekends in middle school with my friend's and we'd play Warcraft 3 late into the night. In high school we moved on to World of Warcraft. I stopped playing sometime toward the end of the Wrath of the Lich King expansion, but started again during the Covid-19 pandemic (so about a 10 year break!).

Upon my return, I found the game had changed quite a bit! One piece that was new to me, and that I found instantly intriguing, was that you could now buy game time using in game gold! For the uninitiated, World of Warcraft is a subscription based game - you have to pay $15/month to play (in the USA anyway, prices are different in different locations). Being able to buy game time with in-game currency was a game changer for me. One of the reasons I quit playing WoW 10 years ago was because I didn't feel like I could justify paying 15 dollars a month when I was a poor undergrad. Now that I'm an only-very-slightly-less poor grad student, 15 dollars a month is still a fair bit of change but if I could pay for it with gold I earned while playing a game I love, that's great! As the goblin characters in WoW will tell you: "Time is money, friend!".

I immediately got to work learning about how others were successful in translating their play time into game time. A lot of players will make their in-game gold playing the Auction House. In WoW, the Auction House is a place where players can (surprise!) auction off in-game items in exchange for currency. There are a variety of professions that players can choose from to gather or produce goods that are useful for other players. Blizzard (the game developer) also happens to have an API that allows you to access the data for all the auctions on a realm's Auction House, and this API is updated every hour. Services exist that tap into this API to help understand the price trends of items for the economically-minded player, but I wanted to level-up my programming skills a bit and build myself a tool that did exactly what I needed. So, I set about learning how to access this API, built a database of auctions, and set up some email alerts for items I was watching so I could buy low and sell high! Since I built this program, I've earned about 8 million in-game gold, which translates to roughly $600 worth of 'Blizzard Balance' (roughly because the gold cost of a 15 dollar token fluctuates over time). That's more than enough money for me to pay for my game time, plus more for me to buy time for friends or spend it on other Blizzard games.

Here I'll walk through the code I used to do this. I wrote the script in an R markdown file so I could weave together R code chunks with Python code chunks. I used both languages in part because I want to learn more Python, so I thought why not, but also because I was having trouble accessing the API using R. Setting it up in Python worked almost flawlessly. I'm a total novice when it comes to scraping APIs, so I'm sure there was something I was doing wrong in R. You'll also see that I use renv to help manage both the R and Python environments - something else I'm learning more about! I've already run into dependency hell with a few R projects, so I'm very thankful for the `renv` package.

```{r, eval = FALSE, echo = TRUE}
Sys.time()

# R imports
.libPaths("path/to/your/project/renv")
library(httr)
library(jsonlite)
library(tidyverse)
library(glue)
library(googledrive)
library(reticulate)
library(lubridate)
library(emayili)
use_condaenv("path/to/your/conda/env")

`%!in%` <- Negate(`%in%`)

# I keep my API keys in this file (make sure you add this file to a .gitignore if you're using git!)
source('data/keys.R')

```

Here I set up the Python functions I'll need to get to the API. If you want to do this, you'll need to make sure you have your region specified.

```{python}
# Set up python functions
import requests
from datetime import datetime
import pandas as pd

# Create a new access token
def create_access_token(client_id, client_secret, region = "us"):
   data = { 'grant_type': 'client_credentials' }
   response = requests.post('https://%s.battle.net/oauth/token' % region,
                            data = data,
                            auth = (client_id, client_secret))
   return response.json()

# Get Malf and connected realms data
def get_malfurion(search):
   search = search
   response = requests.get(search)
   return response.json()["auctions"]
 
response = create_access_token(r['client_id'], r['client_secret'])
token = response['access_token']
```

Now, I specify a few more parameters including my realm ID. I'm on a connected realm for Malfurion (+ a bunch of others I never remember). Each of our realms returns it's own ID, but for some reason this id (1175) was the only one that actually returned data. So if you're on a connected realm, you might have to experiment using other realm's ids.

```{r}
# Set up search query with Malfurion realm ID
id <- 1175
token <- py$token
search <- glue("https://us.api.blizzard.com/data/wow/connected-realm/{id}/auctions?namespace=dynamic-us&locale=en_US&access_token={token}")
```

Now we can actually access the data! I add a few features to the data to capture when I pulled the data. I also write the file to csv. I wrote the file to csv because, even though I should be able to access a Python data frame in R through the reticulate package, I was having trouble getting it to work. My work around was to write the file, reload it so R had access to it, write the R data frame to a SQL database, then delete the csv. You can also see that I do a little bit of cleaning with the data. Depending on the type of item posted, it either has a `unit_price` or a `buyout`, but I want just a single column for cost. I then convert the cost from how it is listed in units of bronze to units of gold (100 bronze in a silver, 100 silver in a gold). 

```{python}
auction_data = get_malfurion(r['search'])
auction_data = pd.DataFrame(auction_data)
# Expand the item column
auction_data = auction_data.rename(columns={"id": "auction_id",})
auction_data = pd.concat([auction_data.drop(['item'], axis=1), auction_data['item'].apply(pd.Series)], axis=1)

# Drop 'bonus_list' and 'modifiers' 
#   These are subgroups of an equipable item with the bonus stats (intellect agility, strength, etc)
auction_data['collection_year'] = datetime.now().strftime('%Y')
auction_data['collection_month'] = datetime.now().strftime('%m')
auction_data['collection_day'] = datetime.now().strftime('%d')
auction_data['collection_hour'] = datetime.now().strftime('%H')
filename = datetime.now().strftime('F:/Documents/WoWAH/data/Malfurion_NA-%Y-%m-%d-%H-%M.csv')
tablename = datetime.now().strftime('Malfurion_NA-%Y-%m-%d-%H-%M')
auction_data.to_csv(filename, index = False)
```

```{r}
# Read the data into R to clean it a bit more (since I'm less good at cleaning)
# in python
auction_df <- read_csv(py$filename)

# Unit prices are for stackable items 
auction_df <- auction_df %>% 
   mutate(unit_price = replace_na(unit_price, 0),
          buyout = replace_na(buyout, 0),
          cost = unit_price + buyout,
          cost_g = cost / 10000)

write_csv(auction_df, py$filename)

# Add to database
library(RSQLite)
library(dbplyr)

con <- dbConnect(RSQLite::SQLite(), "data/WoWAH_database.sqlite")
dbWriteTable(con, py$tablename, auction_df)
file.remove(py$filename) # delete the csv file once it is in the database.
```

The next step was to query the API for information about each item on the auction house. The auction house data doesn't contain the human-readable name for the item, it just has an item ID number. So I have to use a seperate query to get the item information. To do this efficiently and prevent constant excessive querying of the API, I built a new data base table to hold this information and then build on this table if I ever find an item in my most recent auction house scan that isn't in the item info database. 

To do this, I first start by finding all the item ids that aren't already in the data base. Then, I split this list of item ids into chunks with a max of 50 item ids per chunk. I found this number through some trial and error querying the API - any queries requesting information on more than 50 items at a time only returned a max of 50 items. Then I construct as many queries as there are chunks. The first time I ran this, it took a while because I was building the item information database from scratch. But it's pretty uncommon for previously unseen items to show up in the auction house, so this doesn't actually run that often anymore.

```{r}
# Check for any items in the newly scraped AH data that isn't in
# the current database
item_db <- tbl(con, "item_name")
auction_df <- tbl(con, py$tablename)
not_in_db_df <- anti_join(auction_df, item_db, by = "id") %>% 
   collect()
id <- 1175
token <- py$token
# Format item info to request from API
item_ids <- unique(not_in_db_df$id)

# Helper function to chunk id vector
createChunks <- function(x, elements.per.chunk){
   # plain R version
   split(x, rep(seq_along(x), each = elements.per.chunk)[seq_along(x)])

}

item_chunks <- createChunks(item_ids, 50)
search_chunks <- map(item_chunks, function(.x) {
   id_char <- paste(.x, collapse = "||")
   glue("https://us.api.blizzard.com/data/wow/search/item?namespace=static-us&locale=en_US&orderby=id&&_pageSize=1000&id={id_char}&_&access_token={token}")
})
# Need this to be a Python list so I remove names (a named list becomes a python dict)
search_chunks <- unname(search_chunks)
```

Now that I have the item-query-chunks, I use some more Python code to actually do the query.

```{python}
import pandas
item_names = []

for search_url in r['search_chunks']:
   response = requests.get(search_url).json()
   response_df = pandas.io.json.json_normalize(response, record_path = ['results'])
   item_names.append(response_df)


item_df = pd.concat(item_names, ignore_index = True, axis = 0, sort = True)

```

And now I clean up the item table a bit and then write it/append it to the database.

```{r}
# Get Item classes index an
new_item_df <- py$item_df %>% 
   select(data.id, data.is_equippable, data.is_stackable, data.level, data.max_count,
          data.media.id,  data.name.en_US,
          data.purchase_price, data.required_level, data.sell_price) %>% 
   rename_all(~stringr::str_replace(., "^data.", "")) %>% 
   rename_at(.vars = vars(ends_with(".en_US")),
             ~ sub('[.]en_US$', '', .))

dbAppendTable(con, name = "item_name", value = new_item_df)
```

For a while I was running this on a computer with limited storage, so I set it up to delete any data older than 4 weeks. I've since turned off this code chunk, as I moved the script over to a computer with much more storage so I'm not really concerned about the amount of data I'm keeping. 

```{r, eval = FALSE}
# I don't need to keep all the data, let's keep a month's worth
table_list <- data.frame(table_list = dbListTables(con))
table_list <- filter(table_list, table_list %!in% c("item_name", "item_db"))
table_list$date_times <-
   str_extract(table_list$table_list,
               "[0-9]{4}-[0-9]{2}-[0-9]{2}-[0-9]{2}-[0-9]{2}")
table_list$date_times <- ymd_hm(table_list$date_times)
# Just in case the tables aren't in order for date time
table_list$index <- 1:nrow(table_list)
table_list <- arrange(table_list, desc(date_times))
keep_tables <- slice(table_list, 1:672)
remove_tables <- table_list[table_list$index %!in% keep_tables$index, ]
remove_tables <- remove_tables$table_list

map(remove_tables, ~dbRemoveTable(con, .x))
dbDisconnect(con)

```

Lastly, I set up some email alerts! Here I just have an example for two items I was watching: elethium ore and widowbloom. I had to be careful with this, as I initially set the alerts to trigger at prices that occurred to frequently and Google flagged my account for inappropriate use! There's probably some other service I could use and not worry about it, but I had the threshold set too high anyway and was getting more emails than I could act on.

```{r, eval = FALSE}
watched_items <- c('Elethium Ore', 'Widowbloom')
watched_item_df <- auction_df %>% 
   left_join(item_db, by = 'id', copy = TRUE) %>% 
   filter(name %in% watched_items) %>% 
   select(name, cost_g, quantity) %>% 
   group_by(name, cost_g) %>% 
   summarise(quantity = sum(quantity, na.rm = TRUE)) %>% 
   collect()

email_items <- watched_item_df %>% 
   group_by(name) %>% 
   summarise(min_cost = min(cost_g)) %>%
   left_join(., watched_item_df, by = c('name', 'min_cost' = 'cost_g')) %>% 
   mutate(buy = case_when(name == 'Elethium Ore' & min_cost < 60 ~ 1,
                          name == 'Widowbloom' & min_cost < 16 ~ 1,
                          TRUE ~ 0)) %>% 
   filter(buy == 1)
items_to_buy <- paste0(email_items$name, ": ", email_items$quantity, collapse = ", ")
names(items_to_buy) <- 'items_to_buy'
# Set up text to send the giver their assigned gift recipient
if (nrow(email_items) > 0) {
body <- "Dear Jake,

<p>Consider purchasing the following items:
<br> {items_to_buy}

"

# Set up email
email <- envelope()
email <- email %>% 
   from("your.account.for.sending.alerts@email.com") %>% 
   to('your.account.for.receiving@emailcom')

email <- email %>% 
   subject(paste0('WoW AH ', Sys.time()))

email <- email %>% 
   html(glue_data(items_to_buy, body))


smtp <- server(host = "smtp.gmail.com",
               port = 587,
               username = "your.account.for.sending.alerts@email.com",
               password = "your.password.for.email.authentication")
smtp(email, verbose = TRUE)
}
```

And that's it! Hopefully there's enough information here for someone else to get a similar system up and running. Next steps I'd like to do are to build a Shiny dashboard for visualizing some of this data. I'm also interested in learning more about time series analysis, so I'm planning to use this data in some self-teaching exercises. Reach out to me on twitter if you have questions! 




